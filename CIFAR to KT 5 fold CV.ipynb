{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f047e516",
   "metadata": {},
   "source": [
    "## 참고 문서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753f7c2",
   "metadata": {},
   "source": [
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dff01",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d7bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc52832",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/Users/User/303/KT_32px/food_competition_KT_set1/train/'\n",
    "categoris = os.listdir(img_dir)\n",
    "nb_categoris = len(categoris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f74f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for i in range(nb_categoris) :\n",
    "    a = glob.glob(img_dir+'/'+categoris[i]+'/*.jpg')\n",
    "    for j in a :\n",
    "        image=tensorflow.keras.preprocessing.image.load_img(j, color_mode='rgb')\n",
    "        image=np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b0d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=np.array(data)\n",
    "inputs=inputs/255.0\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01516934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "targets=labels.reshape(-1,1)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504579fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9884c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x193215e0ee0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs[9999])\n",
    "#야미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d443ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D,MaxPooling2D,ReLU,LeakyReLU,ELU,BatchNormalization,Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 32\n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 50\n",
    "no_epochs = 50\n",
    "optimizer = Adam()\n",
    "verbosity = 1\n",
    "num_folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0171dba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e38e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('targets.npy',targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa6e6bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "820f2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "act=LeakyReLU(0.01)\n",
    "#act=ReLU()\n",
    "#act=ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e03617c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (128, 128, 3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 5, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, kernel_size = 5,padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(Conv2D(16, kernel_size = 3, activation='relu'))\n",
    "#     #model.add(Conv2D(4, kernel_size = 3, activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(128,activation='relu'))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31d4ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 40~50퍼\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(50, activation='softmax')) # 2 because we have cat and dog classes\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "572d065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #제일 잘나옴\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation=act, input_shape=(32,32,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3), padding='same',activation=act))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3), padding='same',activation=act))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation=act))\n",
    "#     model.add(Dense(128, activation=act))\n",
    "#     model.add(Dense(no_classes, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec247919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 55.8399999%\n",
    "# # Model configuration\n",
    "# # batch_size = 32\n",
    "# # img_width, img_height, img_num_channels = 32, 32, 3\n",
    "# # loss_function = sparse_categorical_crossentropy\n",
    "# # no_classes = 50\n",
    "# # no_epochs = 50\n",
    "# # optimizer = Adam()\n",
    "# # verbosity = 1\n",
    "# # num_folds = 5\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Convolutional Layer\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     # Pooling layer\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     # Dropout layers\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     # model.add(Dropout(0.2))\n",
    "#     model.add(Dense(128, activation=act))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d2f82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "# batch_size = 32\n",
    "# img_width, img_height, img_num_channels = 32, 32, 3\n",
    "# loss_function = sparse_categorical_crossentropy\n",
    "# no_classes = 50\n",
    "# no_epochs = 50\n",
    "# optimizer = Adam()\n",
    "# verbosity = 1\n",
    "# num_folds = 5\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional Layer\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Pooling layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Dropout layers\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation=act))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(50, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43a373e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_251 (Conv2D)         (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_122 (Ba  (None, 32, 32, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_252 (Conv2D)         (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_123 (Ba  (None, 32, 32, 32)       128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_189 (MaxPooli  (None, 16, 16, 32)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_80 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_253 (Conv2D)         (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_124 (Ba  (None, 16, 16, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_254 (Conv2D)         (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_125 (Ba  (None, 16, 16, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_190 (MaxPooli  (None, 8, 8, 64)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_81 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_255 (Conv2D)         (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_126 (Ba  (None, 8, 8, 128)        512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_256 (Conv2D)         (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_127 (Ba  (None, 8, 8, 128)        512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_191 (MaxPooli  (None, 4, 4, 128)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_82 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_257 (Conv2D)         (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_128 (Ba  (None, 4, 4, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_258 (Conv2D)         (None, 4, 4, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_129 (Ba  (None, 4, 4, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling2d_192 (MaxPooli  (None, 2, 2, 256)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " flatten_62 (Flatten)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_160 (Dense)           (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_161 (Dense)           (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,313,746\n",
      "Trainable params: 1,311,826\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0934",
   "metadata": {},
   "source": [
    "### Loss : sparse_categorical_crossentropy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a2227d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_categorical_crossentropy -> target이 int인경우 ->얘가 속도 더 빠르대\n",
    "# categorical_crossentropy -> target이 ont-hot encoding된경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "be236e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 7ms/step - loss: 3.6808 - accuracy: 0.0664 - val_loss: 4.1775 - val_accuracy: 0.0285\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.2261 - accuracy: 0.1110 - val_loss: 3.2611 - val_accuracy: 0.1420\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.9531 - accuracy: 0.1789 - val_loss: 2.7525 - val_accuracy: 0.2090\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.7797 - accuracy: 0.2106 - val_loss: 2.6799 - val_accuracy: 0.2285\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.6134 - accuracy: 0.2615 - val_loss: 2.4714 - val_accuracy: 0.2895\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.5166 - accuracy: 0.2774 - val_loss: 2.5964 - val_accuracy: 0.2705\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.3969 - accuracy: 0.3077 - val_loss: 3.0204 - val_accuracy: 0.2250\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.3248 - accuracy: 0.3254 - val_loss: 2.3465 - val_accuracy: 0.3375\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.2060 - accuracy: 0.3553 - val_loss: 2.3961 - val_accuracy: 0.3155\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.1421 - accuracy: 0.3754 - val_loss: 1.9583 - val_accuracy: 0.4265\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.0532 - accuracy: 0.4001 - val_loss: 2.0412 - val_accuracy: 0.4085\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.9953 - accuracy: 0.4059 - val_loss: 2.1843 - val_accuracy: 0.3850\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.9424 - accuracy: 0.4306 - val_loss: 1.9806 - val_accuracy: 0.4290\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8644 - accuracy: 0.4425 - val_loss: 1.9250 - val_accuracy: 0.4490\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8190 - accuracy: 0.4633 - val_loss: 1.9481 - val_accuracy: 0.4590\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7397 - accuracy: 0.4789 - val_loss: 1.8293 - val_accuracy: 0.4750\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7084 - accuracy: 0.4926 - val_loss: 1.9951 - val_accuracy: 0.4470\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.6417 - accuracy: 0.5064 - val_loss: 2.2710 - val_accuracy: 0.3905\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.6030 - accuracy: 0.5175 - val_loss: 1.7318 - val_accuracy: 0.5050\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.5526 - accuracy: 0.5289 - val_loss: 1.7728 - val_accuracy: 0.5050\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.5010 - accuracy: 0.5454 - val_loss: 1.6961 - val_accuracy: 0.5050\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4373 - accuracy: 0.5565 - val_loss: 2.2550 - val_accuracy: 0.4170\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3887 - accuracy: 0.5658 - val_loss: 1.6863 - val_accuracy: 0.5210\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3315 - accuracy: 0.5871 - val_loss: 1.9766 - val_accuracy: 0.4800\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3072 - accuracy: 0.5890 - val_loss: 2.6183 - val_accuracy: 0.3795\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.2316 - accuracy: 0.6095 - val_loss: 1.7382 - val_accuracy: 0.5245\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2053 - accuracy: 0.6290 - val_loss: 1.7262 - val_accuracy: 0.5380\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1419 - accuracy: 0.6428 - val_loss: 1.8516 - val_accuracy: 0.5020\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.1191 - accuracy: 0.6456 - val_loss: 1.6783 - val_accuracy: 0.5470\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0615 - accuracy: 0.6622 - val_loss: 1.7606 - val_accuracy: 0.5275\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.9994 - accuracy: 0.6744 - val_loss: 1.7665 - val_accuracy: 0.5430\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9951 - accuracy: 0.6820 - val_loss: 1.7227 - val_accuracy: 0.5560\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9391 - accuracy: 0.6974 - val_loss: 1.7002 - val_accuracy: 0.5495\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9171 - accuracy: 0.7105 - val_loss: 1.9709 - val_accuracy: 0.5045\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8672 - accuracy: 0.7197 - val_loss: 1.8658 - val_accuracy: 0.5230\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8106 - accuracy: 0.7408 - val_loss: 1.7864 - val_accuracy: 0.5640\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8168 - accuracy: 0.7411 - val_loss: 2.0126 - val_accuracy: 0.5155\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.7607 - accuracy: 0.7506 - val_loss: 1.7948 - val_accuracy: 0.5595\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.7722 - accuracy: 0.7491 - val_loss: 2.0338 - val_accuracy: 0.5335\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7085 - accuracy: 0.7651 - val_loss: 2.0309 - val_accuracy: 0.5280\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6854 - accuracy: 0.7828 - val_loss: 1.8866 - val_accuracy: 0.5730\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6591 - accuracy: 0.7839 - val_loss: 1.9418 - val_accuracy: 0.5615\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6272 - accuracy: 0.7928 - val_loss: 2.0392 - val_accuracy: 0.5455\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.6211 - accuracy: 0.7976 - val_loss: 1.9242 - val_accuracy: 0.5555\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6072 - accuracy: 0.8034 - val_loss: 2.0166 - val_accuracy: 0.5515\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5721 - accuracy: 0.8131 - val_loss: 2.0667 - val_accuracy: 0.5630\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5399 - accuracy: 0.8217 - val_loss: 2.0142 - val_accuracy: 0.5555\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5289 - accuracy: 0.8314 - val_loss: 1.9796 - val_accuracy: 0.5625\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5106 - accuracy: 0.8326 - val_loss: 2.0601 - val_accuracy: 0.5605\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4966 - accuracy: 0.8382 - val_loss: 2.1198 - val_accuracy: 0.5510\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1198 - accuracy: 0.5510\n",
      "Score for fold 1: loss of 2.11978816986084; accuracy of 55.09999990463257%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 3.5464 - accuracy: 0.0904 - val_loss: 4.5654 - val_accuracy: 0.0695\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 3.0122 - accuracy: 0.1670 - val_loss: 3.2135 - val_accuracy: 0.1255\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.8092 - accuracy: 0.2161 - val_loss: 2.5188 - val_accuracy: 0.2795\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.6690 - accuracy: 0.2432 - val_loss: 2.6082 - val_accuracy: 0.2555\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.5570 - accuracy: 0.2775 - val_loss: 2.4561 - val_accuracy: 0.3215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.4490 - accuracy: 0.3005 - val_loss: 2.3327 - val_accuracy: 0.3200\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.3588 - accuracy: 0.3271 - val_loss: 2.1627 - val_accuracy: 0.3665\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.2279 - accuracy: 0.3530 - val_loss: 2.1725 - val_accuracy: 0.3740\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.1738 - accuracy: 0.3766 - val_loss: 2.2628 - val_accuracy: 0.3770\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 2.0784 - accuracy: 0.3911 - val_loss: 1.9775 - val_accuracy: 0.4260\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.0293 - accuracy: 0.4019 - val_loss: 2.1738 - val_accuracy: 0.3765\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.9339 - accuracy: 0.4181 - val_loss: 2.2395 - val_accuracy: 0.3550\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.8887 - accuracy: 0.4414 - val_loss: 3.1302 - val_accuracy: 0.2465\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.8147 - accuracy: 0.4538 - val_loss: 2.1024 - val_accuracy: 0.3985\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.7676 - accuracy: 0.4710 - val_loss: 1.9306 - val_accuracy: 0.4485\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7176 - accuracy: 0.4754 - val_loss: 2.1962 - val_accuracy: 0.4100\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.6577 - accuracy: 0.4905 - val_loss: 1.9258 - val_accuracy: 0.4490\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.6155 - accuracy: 0.5119 - val_loss: 1.9002 - val_accuracy: 0.4565\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.5554 - accuracy: 0.5275 - val_loss: 1.8581 - val_accuracy: 0.4695\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.4911 - accuracy: 0.5436 - val_loss: 1.9033 - val_accuracy: 0.4575\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4520 - accuracy: 0.5530 - val_loss: 1.7370 - val_accuracy: 0.4870\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.3891 - accuracy: 0.5619 - val_loss: 1.9917 - val_accuracy: 0.4760\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.3599 - accuracy: 0.5755 - val_loss: 1.7729 - val_accuracy: 0.4875\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.3235 - accuracy: 0.5811 - val_loss: 2.0555 - val_accuracy: 0.4445\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2572 - accuracy: 0.6029 - val_loss: 1.8870 - val_accuracy: 0.4870\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2107 - accuracy: 0.6158 - val_loss: 1.8697 - val_accuracy: 0.4870\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1623 - accuracy: 0.6308 - val_loss: 1.8113 - val_accuracy: 0.5040\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0956 - accuracy: 0.6497 - val_loss: 1.9834 - val_accuracy: 0.4890\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0948 - accuracy: 0.6481 - val_loss: 1.9618 - val_accuracy: 0.5070\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0041 - accuracy: 0.6734 - val_loss: 1.7644 - val_accuracy: 0.5250\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0057 - accuracy: 0.6765 - val_loss: 1.9151 - val_accuracy: 0.5175\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.9382 - accuracy: 0.7000 - val_loss: 1.7598 - val_accuracy: 0.5510\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.9202 - accuracy: 0.7023 - val_loss: 1.8864 - val_accuracy: 0.5230\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.8886 - accuracy: 0.7124 - val_loss: 1.8243 - val_accuracy: 0.5380\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.8213 - accuracy: 0.7351 - val_loss: 1.9177 - val_accuracy: 0.5385\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.8150 - accuracy: 0.7334 - val_loss: 1.8774 - val_accuracy: 0.5275\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7465 - accuracy: 0.7570 - val_loss: 1.9082 - val_accuracy: 0.5425\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7602 - accuracy: 0.7530 - val_loss: 2.0879 - val_accuracy: 0.5370\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.7372 - accuracy: 0.7575 - val_loss: 1.9581 - val_accuracy: 0.5200\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6940 - accuracy: 0.7719 - val_loss: 1.9008 - val_accuracy: 0.5500\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6700 - accuracy: 0.7824 - val_loss: 2.2667 - val_accuracy: 0.5230\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6300 - accuracy: 0.7919 - val_loss: 2.0687 - val_accuracy: 0.5445\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6247 - accuracy: 0.7975 - val_loss: 2.1358 - val_accuracy: 0.5125\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5812 - accuracy: 0.8119 - val_loss: 2.1667 - val_accuracy: 0.5415\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6032 - accuracy: 0.8030 - val_loss: 2.0069 - val_accuracy: 0.5420\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5450 - accuracy: 0.8249 - val_loss: 2.0853 - val_accuracy: 0.5670\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5418 - accuracy: 0.8286 - val_loss: 2.1824 - val_accuracy: 0.5525\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5016 - accuracy: 0.8350 - val_loss: 2.2208 - val_accuracy: 0.5610\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5222 - accuracy: 0.8326 - val_loss: 2.0632 - val_accuracy: 0.5500\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5130 - accuracy: 0.8372 - val_loss: 2.4144 - val_accuracy: 0.5395\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4144 - accuracy: 0.5395\n",
      "Score for fold 2: loss of 2.4143691062927246; accuracy of 53.94999980926514%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 3.6368 - accuracy: 0.0880 - val_loss: 4.8192 - val_accuracy: 0.0250\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 3.1323 - accuracy: 0.1587 - val_loss: 3.0593 - val_accuracy: 0.1655\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.8532 - accuracy: 0.2124 - val_loss: 2.8815 - val_accuracy: 0.2015\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.6920 - accuracy: 0.2473 - val_loss: 2.5685 - val_accuracy: 0.2965\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 2.6000 - accuracy: 0.2677 - val_loss: 2.3590 - val_accuracy: 0.3245\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.4799 - accuracy: 0.2957 - val_loss: 2.2727 - val_accuracy: 0.3365\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 2.3818 - accuracy: 0.3285 - val_loss: 2.2779 - val_accuracy: 0.3435\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.3282 - accuracy: 0.3256 - val_loss: 2.2216 - val_accuracy: 0.3550\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.2327 - accuracy: 0.3606 - val_loss: 2.1182 - val_accuracy: 0.3720\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 2.1800 - accuracy: 0.3713 - val_loss: 2.6307 - val_accuracy: 0.3150\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.0618 - accuracy: 0.3913 - val_loss: 2.0179 - val_accuracy: 0.4135\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.9856 - accuracy: 0.4195 - val_loss: 2.2259 - val_accuracy: 0.3735\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.9606 - accuracy: 0.4199 - val_loss: 1.9872 - val_accuracy: 0.4165\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8487 - accuracy: 0.4454 - val_loss: 1.9338 - val_accuracy: 0.4485\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8193 - accuracy: 0.4538 - val_loss: 1.8232 - val_accuracy: 0.4565\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7569 - accuracy: 0.4751 - val_loss: 1.8179 - val_accuracy: 0.4740\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.6804 - accuracy: 0.4913 - val_loss: 1.8690 - val_accuracy: 0.4520\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.6189 - accuracy: 0.5046 - val_loss: 1.8798 - val_accuracy: 0.4590\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.5680 - accuracy: 0.5197 - val_loss: 1.9622 - val_accuracy: 0.4615\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.5009 - accuracy: 0.5381 - val_loss: 1.7772 - val_accuracy: 0.5165\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.4693 - accuracy: 0.5529 - val_loss: 1.9084 - val_accuracy: 0.4780\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.4014 - accuracy: 0.5630 - val_loss: 2.0323 - val_accuracy: 0.4475\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.3667 - accuracy: 0.5748 - val_loss: 2.5459 - val_accuracy: 0.4065\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.3212 - accuracy: 0.5866 - val_loss: 1.8033 - val_accuracy: 0.4870\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2453 - accuracy: 0.6108 - val_loss: 1.7137 - val_accuracy: 0.5150\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2234 - accuracy: 0.6174 - val_loss: 2.0061 - val_accuracy: 0.5050\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.1570 - accuracy: 0.6363 - val_loss: 1.9320 - val_accuracy: 0.4740\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.1150 - accuracy: 0.6445 - val_loss: 1.7507 - val_accuracy: 0.5395\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0757 - accuracy: 0.6620 - val_loss: 1.6833 - val_accuracy: 0.5520\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0521 - accuracy: 0.6655 - val_loss: 1.8777 - val_accuracy: 0.5110\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9939 - accuracy: 0.6790 - val_loss: 1.8862 - val_accuracy: 0.5185\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9645 - accuracy: 0.6841 - val_loss: 1.7663 - val_accuracy: 0.5540\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.9311 - accuracy: 0.6984 - val_loss: 2.2543 - val_accuracy: 0.4760\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.8974 - accuracy: 0.7109 - val_loss: 2.2364 - val_accuracy: 0.4735\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8243 - accuracy: 0.7296 - val_loss: 1.8900 - val_accuracy: 0.5480\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8367 - accuracy: 0.7293 - val_loss: 2.1249 - val_accuracy: 0.4955\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8234 - accuracy: 0.7335 - val_loss: 2.2681 - val_accuracy: 0.4985\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7650 - accuracy: 0.7486 - val_loss: 2.3353 - val_accuracy: 0.5160\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.7564 - accuracy: 0.7548 - val_loss: 1.9790 - val_accuracy: 0.5505\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.7103 - accuracy: 0.7695 - val_loss: 1.9246 - val_accuracy: 0.5720\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6816 - accuracy: 0.7793 - val_loss: 2.0476 - val_accuracy: 0.5550\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6782 - accuracy: 0.7793 - val_loss: 2.3726 - val_accuracy: 0.4855\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.7860 - val_loss: 2.0530 - val_accuracy: 0.5440\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6419 - accuracy: 0.7921 - val_loss: 2.0401 - val_accuracy: 0.5560\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6021 - accuracy: 0.8036 - val_loss: 2.0683 - val_accuracy: 0.5460\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5697 - accuracy: 0.8114 - val_loss: 2.1135 - val_accuracy: 0.5495\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5889 - accuracy: 0.8050 - val_loss: 2.0422 - val_accuracy: 0.5545\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5410 - accuracy: 0.8209 - val_loss: 2.3582 - val_accuracy: 0.5420\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5250 - accuracy: 0.8322 - val_loss: 2.5235 - val_accuracy: 0.5495\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5421 - accuracy: 0.8241 - val_loss: 2.2963 - val_accuracy: 0.5655\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2963 - accuracy: 0.5655\n",
      "Score for fold 3: loss of 2.2962629795074463; accuracy of 56.550002098083496%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 3.6461 - accuracy: 0.0780 - val_loss: 4.3610 - val_accuracy: 0.0775\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 3.0705 - accuracy: 0.1555 - val_loss: 3.0078 - val_accuracy: 0.1675\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.8753 - accuracy: 0.1955 - val_loss: 2.9750 - val_accuracy: 0.1995\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.7064 - accuracy: 0.2345 - val_loss: 2.9147 - val_accuracy: 0.2200\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.5821 - accuracy: 0.2690 - val_loss: 2.7229 - val_accuracy: 0.2570\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 2.4601 - accuracy: 0.2969 - val_loss: 2.3242 - val_accuracy: 0.3270\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 2.3690 - accuracy: 0.3106 - val_loss: 2.7213 - val_accuracy: 0.2790\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.3094 - accuracy: 0.3380 - val_loss: 2.5599 - val_accuracy: 0.2990\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.2188 - accuracy: 0.3615 - val_loss: 2.1732 - val_accuracy: 0.3725\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.1618 - accuracy: 0.3750 - val_loss: 2.2998 - val_accuracy: 0.3520\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.0377 - accuracy: 0.4050 - val_loss: 2.4313 - val_accuracy: 0.3350\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.9887 - accuracy: 0.4155 - val_loss: 1.9104 - val_accuracy: 0.4385\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.9031 - accuracy: 0.4351 - val_loss: 1.9921 - val_accuracy: 0.4145\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.8460 - accuracy: 0.4461 - val_loss: 2.2242 - val_accuracy: 0.3845\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.7837 - accuracy: 0.4615 - val_loss: 1.9101 - val_accuracy: 0.4550\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 8ms/step - loss: 1.7540 - accuracy: 0.4753 - val_loss: 1.9305 - val_accuracy: 0.4570\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.6832 - accuracy: 0.5023 - val_loss: 2.0354 - val_accuracy: 0.4190\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.6190 - accuracy: 0.5017 - val_loss: 2.0331 - val_accuracy: 0.4350\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.5705 - accuracy: 0.5251 - val_loss: 1.8640 - val_accuracy: 0.4805\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.5265 - accuracy: 0.5211 - val_loss: 1.8687 - val_accuracy: 0.4830\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.4498 - accuracy: 0.5523 - val_loss: 1.9054 - val_accuracy: 0.4760\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.4006 - accuracy: 0.5659 - val_loss: 1.8915 - val_accuracy: 0.4765\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3617 - accuracy: 0.5779 - val_loss: 1.8513 - val_accuracy: 0.4820\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3121 - accuracy: 0.5907 - val_loss: 1.7604 - val_accuracy: 0.5230\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.2473 - accuracy: 0.6122 - val_loss: 1.7909 - val_accuracy: 0.5160\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1835 - accuracy: 0.6346 - val_loss: 2.0227 - val_accuracy: 0.4700\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1754 - accuracy: 0.6283 - val_loss: 2.2622 - val_accuracy: 0.4415\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1032 - accuracy: 0.6461 - val_loss: 1.7667 - val_accuracy: 0.5315\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0806 - accuracy: 0.6545 - val_loss: 1.8481 - val_accuracy: 0.5235\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.0376 - accuracy: 0.6659 - val_loss: 1.8097 - val_accuracy: 0.5285\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0192 - accuracy: 0.6773 - val_loss: 1.8124 - val_accuracy: 0.5405\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.9486 - accuracy: 0.6955 - val_loss: 1.7864 - val_accuracy: 0.5380\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.9111 - accuracy: 0.7088 - val_loss: 1.7787 - val_accuracy: 0.5485\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8900 - accuracy: 0.7101 - val_loss: 1.7925 - val_accuracy: 0.5415\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8324 - accuracy: 0.7300 - val_loss: 2.0854 - val_accuracy: 0.4995\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.8076 - accuracy: 0.7311 - val_loss: 1.9597 - val_accuracy: 0.5320\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7863 - accuracy: 0.7421 - val_loss: 1.8952 - val_accuracy: 0.5355\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7455 - accuracy: 0.7526 - val_loss: 2.0225 - val_accuracy: 0.5430\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.7218 - accuracy: 0.7660 - val_loss: 2.0006 - val_accuracy: 0.5295\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7290 - accuracy: 0.7592 - val_loss: 2.0260 - val_accuracy: 0.5355\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7033 - accuracy: 0.7689 - val_loss: 2.0199 - val_accuracy: 0.5300\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6570 - accuracy: 0.7857 - val_loss: 2.1266 - val_accuracy: 0.5230\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.6150 - accuracy: 0.7987 - val_loss: 2.0387 - val_accuracy: 0.5365\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6202 - accuracy: 0.7962 - val_loss: 2.1312 - val_accuracy: 0.5295\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5990 - accuracy: 0.8029 - val_loss: 2.2460 - val_accuracy: 0.5250\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5670 - accuracy: 0.8158 - val_loss: 2.2031 - val_accuracy: 0.5355\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.5376 - accuracy: 0.8213 - val_loss: 2.0373 - val_accuracy: 0.5555\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5416 - accuracy: 0.8184 - val_loss: 2.2573 - val_accuracy: 0.5140\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5237 - accuracy: 0.8299 - val_loss: 2.4157 - val_accuracy: 0.5170\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.4868 - accuracy: 0.8369 - val_loss: 2.1677 - val_accuracy: 0.5555\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1677 - accuracy: 0.5555\n",
      "Score for fold 4: loss of 2.167694568634033; accuracy of 55.549997091293335%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - 3s 8ms/step - loss: 3.6761 - accuracy: 0.0723 - val_loss: 3.8652 - val_accuracy: 0.0565\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 3.1823 - accuracy: 0.1275 - val_loss: 3.7771 - val_accuracy: 0.1100\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.9682 - accuracy: 0.1731 - val_loss: 3.4600 - val_accuracy: 0.1395\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.8332 - accuracy: 0.1986 - val_loss: 2.9434 - val_accuracy: 0.2220\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.6473 - accuracy: 0.2486 - val_loss: 2.8449 - val_accuracy: 0.2080\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.5063 - accuracy: 0.2822 - val_loss: 2.3966 - val_accuracy: 0.3235\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.3766 - accuracy: 0.3124 - val_loss: 2.5617 - val_accuracy: 0.3030\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.2847 - accuracy: 0.3434 - val_loss: 2.2003 - val_accuracy: 0.3510\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.1923 - accuracy: 0.3605 - val_loss: 2.2045 - val_accuracy: 0.3850\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.1180 - accuracy: 0.3879 - val_loss: 2.0620 - val_accuracy: 0.4145\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 2.0546 - accuracy: 0.3969 - val_loss: 2.0620 - val_accuracy: 0.4190\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.9775 - accuracy: 0.4175 - val_loss: 1.9211 - val_accuracy: 0.4320\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 1.9309 - accuracy: 0.4300 - val_loss: 1.8355 - val_accuracy: 0.4650\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8554 - accuracy: 0.4514 - val_loss: 2.0338 - val_accuracy: 0.4185\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.8045 - accuracy: 0.4621 - val_loss: 2.1687 - val_accuracy: 0.4200\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7439 - accuracy: 0.4803 - val_loss: 1.9116 - val_accuracy: 0.4520\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.6859 - accuracy: 0.4875 - val_loss: 2.1412 - val_accuracy: 0.4380\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.6074 - accuracy: 0.5163 - val_loss: 1.7620 - val_accuracy: 0.4975\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.5626 - accuracy: 0.5288 - val_loss: 1.8700 - val_accuracy: 0.4540\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.5121 - accuracy: 0.5320 - val_loss: 2.0297 - val_accuracy: 0.4560\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4860 - accuracy: 0.5412 - val_loss: 1.7504 - val_accuracy: 0.4925\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4072 - accuracy: 0.5709 - val_loss: 1.7261 - val_accuracy: 0.5180\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.3704 - accuracy: 0.5785 - val_loss: 1.9587 - val_accuracy: 0.4755\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.2986 - accuracy: 0.5925 - val_loss: 1.8038 - val_accuracy: 0.4985\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.2636 - accuracy: 0.6018 - val_loss: 1.8251 - val_accuracy: 0.4935\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.2252 - accuracy: 0.6235 - val_loss: 1.8228 - val_accuracy: 0.5140\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1717 - accuracy: 0.6325 - val_loss: 2.1080 - val_accuracy: 0.4600\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.1248 - accuracy: 0.6481 - val_loss: 1.7987 - val_accuracy: 0.5210\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1218 - accuracy: 0.6492 - val_loss: 1.7539 - val_accuracy: 0.5525\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0253 - accuracy: 0.6743 - val_loss: 1.6809 - val_accuracy: 0.5560\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9994 - accuracy: 0.6819 - val_loss: 1.8975 - val_accuracy: 0.5250\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9917 - accuracy: 0.6819 - val_loss: 1.6863 - val_accuracy: 0.5520\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.9372 - accuracy: 0.6977 - val_loss: 2.1786 - val_accuracy: 0.4855\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8984 - accuracy: 0.7134 - val_loss: 1.8075 - val_accuracy: 0.5585\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8789 - accuracy: 0.7207 - val_loss: 1.9556 - val_accuracy: 0.5360\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8035 - accuracy: 0.7459 - val_loss: 1.7484 - val_accuracy: 0.5675\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.8132 - accuracy: 0.7400 - val_loss: 1.7943 - val_accuracy: 0.5535\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7946 - accuracy: 0.7490 - val_loss: 1.7965 - val_accuracy: 0.5815\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7472 - accuracy: 0.7584 - val_loss: 1.8727 - val_accuracy: 0.5490\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7278 - accuracy: 0.7657 - val_loss: 1.8788 - val_accuracy: 0.5555\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.7274 - accuracy: 0.7632 - val_loss: 1.8664 - val_accuracy: 0.5570\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6653 - accuracy: 0.7901 - val_loss: 2.0402 - val_accuracy: 0.5525\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6489 - accuracy: 0.7891 - val_loss: 2.0517 - val_accuracy: 0.5375\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6392 - accuracy: 0.7946 - val_loss: 2.0157 - val_accuracy: 0.5685\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.6183 - accuracy: 0.8086 - val_loss: 1.9072 - val_accuracy: 0.5630\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5783 - accuracy: 0.8179 - val_loss: 1.9955 - val_accuracy: 0.5645\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5431 - accuracy: 0.8200 - val_loss: 2.0549 - val_accuracy: 0.5560\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5520 - accuracy: 0.8173 - val_loss: 2.0568 - val_accuracy: 0.5510\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5630 - accuracy: 0.8149 - val_loss: 2.0727 - val_accuracy: 0.5745\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.5406 - accuracy: 0.8246 - val_loss: 2.1755 - val_accuracy: 0.5530\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1755 - accuracy: 0.5530\n",
      "Score for fold 5: loss of 2.1754817962646484; accuracy of 55.299997329711914%\n"
     ]
    }
   ],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True,random_state = 5)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    #print(train)\n",
    "    #print(test)\n",
    "    model=model_fn()\n",
    "    # Compile the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=no_epochs,\n",
    "                  validation_data=(inputs[test], targets[test]),\n",
    "                  verbose=verbosity)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=1)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fafce3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 2.11978816986084 - Accuracy: 55.09999990463257%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.4143691062927246 - Accuracy: 53.94999980926514%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 2.2962629795074463 - Accuracy: 56.550002098083496%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 2.167694568634033 - Accuracy: 55.549997091293335%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.1754817962646484 - Accuracy: 55.299997329711914%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 55.28999924659729 (+- 0.8351052949973962)\n",
      "> Loss: 2.2347193241119383\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
