{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f047e516",
   "metadata": {},
   "source": [
    "## 참고 문서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753f7c2",
   "metadata": {},
   "source": [
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dff01",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1d7bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc52832",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/Users/User/303/KT_32px/food_competition_KT_set1/train/'\n",
    "categoris = os.listdir(img_dir)\n",
    "nb_categoris = len(categoris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f74f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for i in range(nb_categoris) :\n",
    "    a = glob.glob(img_dir+'/'+categoris[i]+'/*.jpg')\n",
    "    for j in a :\n",
    "        image=tensorflow.keras.preprocessing.image.load_img(j, color_mode='rgb')\n",
    "        image=np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b0d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=np.array(data)\n",
    "inputs=inputs/255.0\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01516934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "targets=labels.reshape(-1,1)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504579fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9884c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2dbe3360730>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs[9999])\n",
    "#야미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d443ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D,MaxPooling2D,ReLU,LeakyReLU,ELU,BatchNormalization,Dropout,GlobalAveragePooling2D,Input,AveragePooling2D, Activation\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam,SGD,Nadam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "\n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 50\n",
    "verbosity = 1\n",
    "num_folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0171dba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e38e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('targets.npy',targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6e6bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820f2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#act=LeakyReLU(0.01)\n",
    "act=ReLU()\n",
    "#act=ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55919f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #61~63퍼 제일 잘나옴\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Conv2D(64, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Conv2D(128, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(128, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a9c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#65퍼 Best batch32 epoch 200 adam()\n",
    "def model_fn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(256, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation=act, kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e821d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #63.8%\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(64, (3, 3), activation=act,  padding='same', input_shape=(32, 32, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, (3, 3), activation=act,padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Conv2D(128, (3, 3), activation=act,  padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(128, (3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Conv2D(256, (3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(256, (3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Conv2D(512, (3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(512, (3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Conv2D(1024, (3, 3), activation=act,  padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(1024, (3, 3), activation=act,  padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool2D((2, 2)))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc5deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder\n",
    "# def model_fn():\n",
    "#     input_img = Input(shape=(32,32,3))\n",
    "#     conv1 = Conv2D(32, (3,3), activation=act, padding='same',name='encoder1_cov')(input_img) \n",
    "#     pool1 = MaxPooling2D(pool_size=(2,2),name='maxpool1')(conv1) \n",
    "#     conv2 = Conv2D(32, (3,3), activation=act, padding='same',name='encoder2_cov')(pool1)\n",
    "#     pool2 = MaxPooling2D(pool_size=(2,2),name='maxpool2')(conv2) \n",
    "#     encoded = Conv2D(64, (3,3), activation=act, padding='same',name='encoder3_cov')(pool2) \n",
    "\n",
    "#     # Decoder1\n",
    "#     conv4 = Conv2D(64, (3,3), activation=act, padding='same',name='decoder1_cov')(encoded)\n",
    "#     #up1 = UpSampling2D((2,2),name='up1')(conv4)\n",
    "#     conv5 = Conv2D(32, (3,3), activation=act, padding='same',name='decoder2_cov')(up1)\n",
    "#     #up2 = UpSampling2D((2,2),name='up2')(conv5)\n",
    "\n",
    "#     # Decoder2\n",
    "#     conv6 = Conv2D(64, (3,3), activation=act, padding='same',name='decoder3_cov')(encoded)\n",
    "#     #up3 = UpSampling2D((2,2),name='up3')(conv6)\n",
    "#     conv7 = Conv2D(32, (3,3), activation=act, padding='same',name='decoder4_cov')(up3)\n",
    "#     #up4 = UpSampling2D((2,2),name='up4')(conv7)\n",
    "#     #Merge\n",
    "#     con=Concatenate()([up2,up4])\n",
    "#     conv8 = Conv2D(64, (3,3), activation=act, padding='same',name='decoder5_cov')(con)\n",
    "#     decoded = Conv2D(1, (3,3), activation='sigmoid', padding='same',name='result_cov')(conv8)\n",
    "#     model = Model(input_img, decoded)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a373e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,314,258\n",
      "Trainable params: 1,312,082\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb6a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #best\n",
    "# def lr_schedule(epoch):\n",
    "#     lr = 1e-3\n",
    "#     if epoch > 180:\n",
    "#         lr *= 0.5e-3\n",
    "#     elif epoch > 160:\n",
    "#         lr *= 1e-3\n",
    "#     elif epoch > 120:\n",
    "#         lr *= 1e-2\n",
    "#     elif epoch > 80:\n",
    "#         lr *= 1e-1\n",
    "#     print('Learning rate: ', lr)\n",
    "#     return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1570202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 150:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 100:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 50:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d48780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0934",
   "metadata": {},
   "source": [
    "### Loss : sparse_categorical_crossentropy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2227d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_categorical_crossentropy -> target이 int인경우 ->얘가 속도 더 빠르대\n",
    "# categorical_crossentropy -> target이 ont-hot encoding된경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6c8d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32나 128이나 비슷 128이 속도 더 빨라서 결과내기 수월\n",
    "no_epochs = 200\n",
    "optimizer = Adam(0.001)\n",
    "#optimizer = Nadam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be236e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 19ms/step - loss: 4.0636 - accuracy: 0.0776 - val_loss: 4.4068 - val_accuracy: 0.0290 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 3.1880 - accuracy: 0.1680 - val_loss: 4.8075 - val_accuracy: 0.0440 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.7862 - accuracy: 0.2380 - val_loss: 5.0310 - val_accuracy: 0.0640 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.5625 - accuracy: 0.2859 - val_loss: 4.8977 - val_accuracy: 0.0890 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.4017 - accuracy: 0.3214 - val_loss: 4.3062 - val_accuracy: 0.1250 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.2850 - accuracy: 0.3500 - val_loss: 3.5991 - val_accuracy: 0.1730 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.1700 - accuracy: 0.3760 - val_loss: 2.3957 - val_accuracy: 0.3415 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.1069 - accuracy: 0.3893 - val_loss: 2.2697 - val_accuracy: 0.3725 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.0007 - accuracy: 0.4212 - val_loss: 2.0164 - val_accuracy: 0.4235 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.9316 - accuracy: 0.4358 - val_loss: 2.0112 - val_accuracy: 0.4155 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.8747 - accuracy: 0.4544 - val_loss: 2.0480 - val_accuracy: 0.4160 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.8096 - accuracy: 0.4703 - val_loss: 1.9486 - val_accuracy: 0.4630 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.7387 - accuracy: 0.4846 - val_loss: 1.8694 - val_accuracy: 0.4735 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.6891 - accuracy: 0.5016 - val_loss: 1.7451 - val_accuracy: 0.4885 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.6483 - accuracy: 0.5024 - val_loss: 1.7860 - val_accuracy: 0.4785 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.6138 - accuracy: 0.5219 - val_loss: 1.7922 - val_accuracy: 0.4800 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.5579 - accuracy: 0.5356 - val_loss: 1.6547 - val_accuracy: 0.5255 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.5180 - accuracy: 0.5471 - val_loss: 1.7988 - val_accuracy: 0.4930 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.4629 - accuracy: 0.5587 - val_loss: 1.7610 - val_accuracy: 0.4875 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.3977 - accuracy: 0.5766 - val_loss: 1.6084 - val_accuracy: 0.5250 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.3701 - accuracy: 0.5854 - val_loss: 1.6102 - val_accuracy: 0.5280 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.3397 - accuracy: 0.5867 - val_loss: 1.7119 - val_accuracy: 0.5215 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.3015 - accuracy: 0.6043 - val_loss: 1.7772 - val_accuracy: 0.4955 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.2440 - accuracy: 0.6200 - val_loss: 1.5612 - val_accuracy: 0.5490 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.2166 - accuracy: 0.6270 - val_loss: 1.5633 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.1708 - accuracy: 0.6344 - val_loss: 1.5114 - val_accuracy: 0.5620 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.1482 - accuracy: 0.6463 - val_loss: 1.6491 - val_accuracy: 0.5560 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 1.1044 - accuracy: 0.6541 - val_loss: 1.5360 - val_accuracy: 0.5700 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.0884 - accuracy: 0.6649 - val_loss: 1.6239 - val_accuracy: 0.5460 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.0372 - accuracy: 0.6773 - val_loss: 1.9105 - val_accuracy: 0.4940 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 1.0028 - accuracy: 0.6892 - val_loss: 1.5895 - val_accuracy: 0.5595 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.9647 - accuracy: 0.6981 - val_loss: 1.5160 - val_accuracy: 0.5890 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.9263 - accuracy: 0.7086 - val_loss: 1.5240 - val_accuracy: 0.5785 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.9145 - accuracy: 0.7100 - val_loss: 1.5608 - val_accuracy: 0.5855 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.8496 - accuracy: 0.7318 - val_loss: 1.6627 - val_accuracy: 0.5615 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.8265 - accuracy: 0.7369 - val_loss: 1.5439 - val_accuracy: 0.6020 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.8236 - accuracy: 0.7420 - val_loss: 1.5715 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.8086 - accuracy: 0.7496 - val_loss: 1.5204 - val_accuracy: 0.5975 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.7713 - accuracy: 0.7519 - val_loss: 1.6307 - val_accuracy: 0.5795 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.7358 - accuracy: 0.7706 - val_loss: 1.5594 - val_accuracy: 0.5875 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.6957 - accuracy: 0.7835 - val_loss: 1.5014 - val_accuracy: 0.6235 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6688 - accuracy: 0.7835 - val_loss: 1.6660 - val_accuracy: 0.5955 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6603 - accuracy: 0.7895 - val_loss: 1.5874 - val_accuracy: 0.6025 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6532 - accuracy: 0.7893 - val_loss: 1.6069 - val_accuracy: 0.6185 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.6254 - accuracy: 0.8000 - val_loss: 1.6711 - val_accuracy: 0.6005 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5839 - accuracy: 0.8139 - val_loss: 1.7327 - val_accuracy: 0.5750 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5841 - accuracy: 0.8156 - val_loss: 1.6830 - val_accuracy: 0.5960 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5631 - accuracy: 0.8201 - val_loss: 1.6269 - val_accuracy: 0.6110 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5661 - accuracy: 0.8210 - val_loss: 1.8654 - val_accuracy: 0.5705 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5471 - accuracy: 0.8261 - val_loss: 1.7107 - val_accuracy: 0.5990 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.5090 - accuracy: 0.8311 - val_loss: 1.7545 - val_accuracy: 0.5925 - lr: 3.1623e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.4194 - accuracy: 0.8680 - val_loss: 1.5574 - val_accuracy: 0.6250 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3667 - accuracy: 0.8896 - val_loss: 1.5164 - val_accuracy: 0.6295 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.3575 - accuracy: 0.8898 - val_loss: 1.5156 - val_accuracy: 0.6260 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3405 - accuracy: 0.8916 - val_loss: 1.5424 - val_accuracy: 0.6280 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3437 - accuracy: 0.8910 - val_loss: 1.5217 - val_accuracy: 0.6305 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3145 - accuracy: 0.9014 - val_loss: 1.5083 - val_accuracy: 0.6300 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3222 - accuracy: 0.8971 - val_loss: 1.5256 - val_accuracy: 0.6350 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3086 - accuracy: 0.9039 - val_loss: 1.5391 - val_accuracy: 0.6310 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3080 - accuracy: 0.9005 - val_loss: 1.5185 - val_accuracy: 0.6370 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.3022 - accuracy: 0.9068 - val_loss: 1.5249 - val_accuracy: 0.6340 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2846 - accuracy: 0.9124 - val_loss: 1.5277 - val_accuracy: 0.6345 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2776 - accuracy: 0.9149 - val_loss: 1.5344 - val_accuracy: 0.6350 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2810 - accuracy: 0.9112 - val_loss: 1.5340 - val_accuracy: 0.6325 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2819 - accuracy: 0.9130 - val_loss: 1.5466 - val_accuracy: 0.6310 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2668 - accuracy: 0.9162 - val_loss: 1.5419 - val_accuracy: 0.6345 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2532 - accuracy: 0.9215 - val_loss: 1.5493 - val_accuracy: 0.6340 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2633 - accuracy: 0.9219 - val_loss: 1.5748 - val_accuracy: 0.6310 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2569 - accuracy: 0.9186 - val_loss: 1.5698 - val_accuracy: 0.6360 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2643 - accuracy: 0.9181 - val_loss: 1.5909 - val_accuracy: 0.6375 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2569 - accuracy: 0.9185 - val_loss: 1.5758 - val_accuracy: 0.6405 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2504 - accuracy: 0.9230 - val_loss: 1.5643 - val_accuracy: 0.6410 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2404 - accuracy: 0.9275 - val_loss: 1.5489 - val_accuracy: 0.6415 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2332 - accuracy: 0.9300 - val_loss: 1.5588 - val_accuracy: 0.6375 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2366 - accuracy: 0.9317 - val_loss: 1.5841 - val_accuracy: 0.6340 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2266 - accuracy: 0.9295 - val_loss: 1.5989 - val_accuracy: 0.6400 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2369 - accuracy: 0.9259 - val_loss: 1.5837 - val_accuracy: 0.6450 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2352 - accuracy: 0.9273 - val_loss: 1.5873 - val_accuracy: 0.6345 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2183 - accuracy: 0.9342 - val_loss: 1.5971 - val_accuracy: 0.6375 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2248 - accuracy: 0.9294 - val_loss: 1.6193 - val_accuracy: 0.6320 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2306 - accuracy: 0.9268 - val_loss: 1.6237 - val_accuracy: 0.6395 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2155 - accuracy: 0.9320 - val_loss: 1.5981 - val_accuracy: 0.6400 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2096 - accuracy: 0.9369 - val_loss: 1.6041 - val_accuracy: 0.6380 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2235 - accuracy: 0.9325 - val_loss: 1.6062 - val_accuracy: 0.6475 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2162 - accuracy: 0.9337 - val_loss: 1.6317 - val_accuracy: 0.6420 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.2099 - accuracy: 0.9352 - val_loss: 1.6067 - val_accuracy: 0.6400 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2026 - accuracy: 0.9380 - val_loss: 1.6234 - val_accuracy: 0.6380 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2030 - accuracy: 0.9389 - val_loss: 1.6579 - val_accuracy: 0.6380 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1890 - accuracy: 0.9425 - val_loss: 1.6367 - val_accuracy: 0.6360 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2011 - accuracy: 0.9389 - val_loss: 1.6494 - val_accuracy: 0.6395 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1874 - accuracy: 0.9450 - val_loss: 1.6663 - val_accuracy: 0.6335 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1920 - accuracy: 0.9392 - val_loss: 1.6587 - val_accuracy: 0.6410 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1841 - accuracy: 0.9484 - val_loss: 1.6417 - val_accuracy: 0.6415 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1863 - accuracy: 0.9431 - val_loss: 1.6552 - val_accuracy: 0.6425 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1900 - accuracy: 0.9414 - val_loss: 1.6782 - val_accuracy: 0.6400 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1815 - accuracy: 0.9438 - val_loss: 1.6530 - val_accuracy: 0.6380 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1822 - accuracy: 0.9444 - val_loss: 1.6663 - val_accuracy: 0.6350 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1692 - accuracy: 0.9451 - val_loss: 1.6945 - val_accuracy: 0.6365 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1728 - accuracy: 0.9482 - val_loss: 1.6842 - val_accuracy: 0.6340 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1769 - accuracy: 0.9439 - val_loss: 1.6933 - val_accuracy: 0.6355 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1783 - accuracy: 0.9455 - val_loss: 1.6814 - val_accuracy: 0.6385 - lr: 3.1623e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1674 - accuracy: 0.9498 - val_loss: 1.6798 - val_accuracy: 0.6420 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1742 - accuracy: 0.9482 - val_loss: 1.6819 - val_accuracy: 0.6450 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1652 - accuracy: 0.9488 - val_loss: 1.6819 - val_accuracy: 0.6430 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1628 - accuracy: 0.9504 - val_loss: 1.6802 - val_accuracy: 0.6460 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1623 - accuracy: 0.9523 - val_loss: 1.6777 - val_accuracy: 0.6445 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1649 - accuracy: 0.9477 - val_loss: 1.6796 - val_accuracy: 0.6445 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1599 - accuracy: 0.9544 - val_loss: 1.6824 - val_accuracy: 0.6440 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1629 - accuracy: 0.9521 - val_loss: 1.6847 - val_accuracy: 0.6455 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1622 - accuracy: 0.9516 - val_loss: 1.6824 - val_accuracy: 0.6445 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1625 - accuracy: 0.9496 - val_loss: 1.6788 - val_accuracy: 0.6440 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1677 - accuracy: 0.9504 - val_loss: 1.6803 - val_accuracy: 0.6445 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1633 - accuracy: 0.9523 - val_loss: 1.6793 - val_accuracy: 0.6445 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1618 - accuracy: 0.9491 - val_loss: 1.6791 - val_accuracy: 0.6450 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1702 - accuracy: 0.9471 - val_loss: 1.6816 - val_accuracy: 0.6425 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1557 - accuracy: 0.9519 - val_loss: 1.6832 - val_accuracy: 0.6435 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1616 - accuracy: 0.9513 - val_loss: 1.6832 - val_accuracy: 0.6430 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1723 - accuracy: 0.9471 - val_loss: 1.6865 - val_accuracy: 0.6435 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1627 - accuracy: 0.9488 - val_loss: 1.6856 - val_accuracy: 0.6420 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1511 - accuracy: 0.9548 - val_loss: 1.6891 - val_accuracy: 0.6435 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1566 - accuracy: 0.9520 - val_loss: 1.6890 - val_accuracy: 0.6460 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1562 - accuracy: 0.9544 - val_loss: 1.6871 - val_accuracy: 0.6435 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1546 - accuracy: 0.9575 - val_loss: 1.6857 - val_accuracy: 0.6435 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1590 - accuracy: 0.9496 - val_loss: 1.6892 - val_accuracy: 0.6430 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1589 - accuracy: 0.9509 - val_loss: 1.6912 - val_accuracy: 0.6435 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1622 - accuracy: 0.9523 - val_loss: 1.6904 - val_accuracy: 0.6440 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1515 - accuracy: 0.9559 - val_loss: 1.6900 - val_accuracy: 0.6445 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1598 - accuracy: 0.9529 - val_loss: 1.6887 - val_accuracy: 0.6450 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1506 - accuracy: 0.9566 - val_loss: 1.6911 - val_accuracy: 0.6475 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1534 - accuracy: 0.9557 - val_loss: 1.6914 - val_accuracy: 0.6470 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1637 - accuracy: 0.9491 - val_loss: 1.6923 - val_accuracy: 0.6480 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1539 - accuracy: 0.9542 - val_loss: 1.6928 - val_accuracy: 0.6470 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1570 - accuracy: 0.9540 - val_loss: 1.6958 - val_accuracy: 0.6455 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1553 - accuracy: 0.9532 - val_loss: 1.6904 - val_accuracy: 0.6455 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1617 - accuracy: 0.9499 - val_loss: 1.6888 - val_accuracy: 0.6440 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1535 - accuracy: 0.9559 - val_loss: 1.6838 - val_accuracy: 0.6470 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1585 - accuracy: 0.9519 - val_loss: 1.6898 - val_accuracy: 0.6485 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1446 - accuracy: 0.9581 - val_loss: 1.6881 - val_accuracy: 0.6455 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1532 - accuracy: 0.9549 - val_loss: 1.6870 - val_accuracy: 0.6450 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1588 - accuracy: 0.9549 - val_loss: 1.6871 - val_accuracy: 0.6490 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1583 - accuracy: 0.9525 - val_loss: 1.6859 - val_accuracy: 0.6485 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1544 - accuracy: 0.9516 - val_loss: 1.6882 - val_accuracy: 0.6475 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1479 - accuracy: 0.9564 - val_loss: 1.6905 - val_accuracy: 0.6465 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1570 - accuracy: 0.9513 - val_loss: 1.6902 - val_accuracy: 0.6460 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1462 - accuracy: 0.9579 - val_loss: 1.6910 - val_accuracy: 0.6470 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1538 - accuracy: 0.9540 - val_loss: 1.6910 - val_accuracy: 0.6445 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1457 - accuracy: 0.9542 - val_loss: 1.6937 - val_accuracy: 0.6455 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1502 - accuracy: 0.9566 - val_loss: 1.6950 - val_accuracy: 0.6465 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1510 - accuracy: 0.9544 - val_loss: 1.6954 - val_accuracy: 0.6475 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1535 - accuracy: 0.9540 - val_loss: 1.6956 - val_accuracy: 0.6500 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1598 - accuracy: 0.9485 - val_loss: 1.6941 - val_accuracy: 0.6485 - lr: 3.1623e-06\n",
      "Learning rate:  5e-07\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1495 - accuracy: 0.9540 - val_loss: 1.6925 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1629 - accuracy: 0.9490 - val_loss: 1.6925 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1493 - accuracy: 0.9584 - val_loss: 1.6914 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1484 - accuracy: 0.9545 - val_loss: 1.6927 - val_accuracy: 0.6470 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1433 - accuracy: 0.9599 - val_loss: 1.6918 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1479 - accuracy: 0.9566 - val_loss: 1.6936 - val_accuracy: 0.6470 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1465 - accuracy: 0.9566 - val_loss: 1.6912 - val_accuracy: 0.6490 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1542 - accuracy: 0.9529 - val_loss: 1.6909 - val_accuracy: 0.6480 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1486 - accuracy: 0.9555 - val_loss: 1.6913 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1518 - accuracy: 0.9564 - val_loss: 1.6919 - val_accuracy: 0.6485 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1561 - accuracy: 0.9532 - val_loss: 1.6924 - val_accuracy: 0.6470 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1647 - accuracy: 0.9476 - val_loss: 1.6915 - val_accuracy: 0.6480 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1617 - accuracy: 0.9509 - val_loss: 1.6930 - val_accuracy: 0.6485 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1589 - accuracy: 0.9530 - val_loss: 1.6918 - val_accuracy: 0.6485 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1442 - accuracy: 0.9582 - val_loss: 1.6911 - val_accuracy: 0.6485 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1418 - accuracy: 0.9595 - val_loss: 1.6897 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1501 - accuracy: 0.9554 - val_loss: 1.6911 - val_accuracy: 0.6475 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1603 - accuracy: 0.9523 - val_loss: 1.6912 - val_accuracy: 0.6480 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1516 - accuracy: 0.9539 - val_loss: 1.6912 - val_accuracy: 0.6480 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1496 - accuracy: 0.9571 - val_loss: 1.6898 - val_accuracy: 0.6490 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1463 - accuracy: 0.9575 - val_loss: 1.6888 - val_accuracy: 0.6480 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1467 - accuracy: 0.9580 - val_loss: 1.6905 - val_accuracy: 0.6485 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 174/200\n",
      "22/63 [=========>....................] - ETA: 0s - loss: 0.1393 - accuracy: 0.9577"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Fit data to model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Generate generalization metrics\u001b[39;00m\n\u001b[0;32m     34\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(inputs[test], targets[test], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True,random_state = 5)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    #print(train)\n",
    "    #print(test)\n",
    "    # Compile the model\n",
    "    model=model_fn()\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                optimizer=optimizer,\n",
    "               \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "                  epochs=no_epochs,\n",
    "                  validation_data=(inputs[test], targets[test]),\n",
    "                         batch_size=batch_size,\n",
    "                  verbose=verbosity,\n",
    "                       callbacks=callbacks)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=1)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafce3c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
