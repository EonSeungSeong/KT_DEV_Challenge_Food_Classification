{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f047e516",
   "metadata": {},
   "source": [
    "## 참고 문서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753f7c2",
   "metadata": {},
   "source": [
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dff01",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d7bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc52832",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/Users/User/303/KT_32px/food_competition_KT_set1/train/'\n",
    "categoris = os.listdir(img_dir)\n",
    "nb_categoris = len(categoris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f74f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for i in range(nb_categoris) :\n",
    "    a = glob.glob(img_dir+'/'+categoris[i]+'/*.jpg')\n",
    "    for j in a :\n",
    "        image=tensorflow.keras.preprocessing.image.load_img(j, color_mode='rgb')\n",
    "        image=np.array(image)\n",
    "        data.append(image)\n",
    "        labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b0d369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=np.array(data)\n",
    "inputs=inputs/255.0\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01516934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "targets=labels.reshape(-1,1)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9884c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e0290dbdc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(inputs[9999])\n",
    "#야미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d443ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,  Activation,Flatten, Conv2D,MaxPooling2D,ReLU,LeakyReLU,ELU,BatchNormalization,Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "# Model configuration\n",
    "batch_size = 32\n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 50\n",
    "no_epochs = 50\n",
    "optimizer = Adam()\n",
    "verbosity = 1\n",
    "num_folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0171dba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e38e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('targets.npy',targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6e6bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820f2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "act=LeakyReLU(0.01)\n",
    "#act=ReLU()\n",
    "#act=ELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e03617c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (128, 128, 3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 5, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(64, kernel_size = 5,padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "#     model.add(Conv2D(16, kernel_size = 3, activation='relu'))\n",
    "#     #model.add(Conv2D(4, kernel_size = 3, activation='relu'))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(128,activation='relu'))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d4ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 40~50퍼\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(50, activation='softmax')) # 2 because we have cat and dog classes\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572d065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #제일 잘나옴\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation=act, input_shape=(32,32,3)))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(64, kernel_size=(3, 3), padding='same',activation=act))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3), padding='same',activation=act))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(256, activation=act))\n",
    "#     model.add(Dense(128, activation=act))\n",
    "#     model.add(Dense(no_classes, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec247919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 55.8399999%\n",
    "# # Model configuration\n",
    "# # batch_size = 32\n",
    "# # img_width, img_height, img_num_channels = 32, 32, 3\n",
    "# # loss_function = sparse_categorical_crossentropy\n",
    "# # no_classes = 50\n",
    "# # no_epochs = 50\n",
    "# # optimizer = Adam()\n",
    "# # verbosity = 1\n",
    "# # num_folds = 5\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Convolutional Layer\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     # Pooling layer\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     # Dropout layers\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     # model.add(Dropout(0.2))\n",
    "#     model.add(Dense(128, activation=act))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1704c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_fn():\n",
    "    \n",
    "    model = Sequential()\n",
    "    weight_decay = 0.0005\n",
    "    weight_decay = weight_decay\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                     input_shape=(32,32,3),kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(no_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d2f82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model configuration\n",
    "# # batch_size = 32\n",
    "# # img_width, img_height, img_num_channels = 32, 32, 3\n",
    "# # loss_function = sparse_categorical_crossentropy\n",
    "# # no_classes = 50\n",
    "# # no_epochs = 50\n",
    "# # optimizer = Adam()\n",
    "# # verbosity = 1\n",
    "# # num_folds = 5\n",
    "# def model_fn():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # Convolutional Layer\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=32, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     # Pooling layer\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     # Dropout layers\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=64, kernel_size=(3, 3),activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=128, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Conv2D(filters=256, kernel_size=(3, 3), activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Conv2D(filters=256, kernel_size=(3, 3),  activation=act, padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     # model.add(Dropout(0.2))\n",
    "#     model.add(Dense(128, activation=act))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Dense(50, activation='softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43a373e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 8, 8, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 4, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 4, 4, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 4, 4, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 2, 2, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                25650     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 50)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,021,938\n",
      "Trainable params: 15,012,466\n",
      "Non-trainable params: 9,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=model_fn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0934",
   "metadata": {},
   "source": [
    "### Loss : sparse_categorical_crossentropy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a2227d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_categorical_crossentropy -> target이 int인경우 ->얘가 속도 더 빠르대\n",
    "# categorical_crossentropy -> target이 ont-hot encoding된경우 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dd70a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "maxepoches = 200\n",
    "learning_rate = 0.1\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7763d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be236e8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 31ms/step - loss: 24.5725 - accuracy: 0.0428 - val_loss: 26525.6973 - val_accuracy: 0.0260 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 26.3883 - accuracy: 0.0855 - val_loss: 318.3046 - val_accuracy: 0.0395 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 23.3907 - accuracy: 0.1079 - val_loss: 22.2381 - val_accuracy: 0.1155 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 21.0083 - accuracy: 0.1285 - val_loss: 20.8792 - val_accuracy: 0.0305 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 18.7346 - accuracy: 0.1450 - val_loss: 19.5192 - val_accuracy: 0.0210 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 16.8009 - accuracy: 0.1614 - val_loss: 17.5755 - val_accuracy: 0.0170 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 15.0434 - accuracy: 0.1838 - val_loss: 16.3352 - val_accuracy: 0.0220 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 13.5341 - accuracy: 0.1940 - val_loss: 15.0661 - val_accuracy: 0.0220 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 12.2647 - accuracy: 0.2056 - val_loss: 13.9065 - val_accuracy: 0.0285 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 11.0947 - accuracy: 0.2236 - val_loss: 12.7635 - val_accuracy: 0.0390 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 10.0611 - accuracy: 0.2345 - val_loss: 11.5079 - val_accuracy: 0.0335 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 9.1573 - accuracy: 0.2403 - val_loss: 10.8660 - val_accuracy: 0.0235 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 8.3490 - accuracy: 0.2544 - val_loss: 10.1200 - val_accuracy: 0.0240 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 7.6455 - accuracy: 0.2760 - val_loss: 9.4179 - val_accuracy: 0.0555 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 7.0413 - accuracy: 0.2784 - val_loss: 8.8380 - val_accuracy: 0.0590 - lr: 0.1000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.4847 - accuracy: 0.2965 - val_loss: 8.2381 - val_accuracy: 0.0550 - lr: 0.1000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.0403 - accuracy: 0.3018 - val_loss: 8.7211 - val_accuracy: 0.0545 - lr: 0.1000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.5779 - accuracy: 0.3214 - val_loss: 7.3658 - val_accuracy: 0.0970 - lr: 0.1000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.1834 - accuracy: 0.3368 - val_loss: 6.3873 - val_accuracy: 0.1215 - lr: 0.1000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 4.8724 - accuracy: 0.3397 - val_loss: 6.5017 - val_accuracy: 0.0765 - lr: 0.1000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.5448 - accuracy: 0.3671 - val_loss: 5.6988 - val_accuracy: 0.1390 - lr: 0.0500\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.3284 - accuracy: 0.3882 - val_loss: 5.8717 - val_accuracy: 0.1125 - lr: 0.0500\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.2033 - accuracy: 0.3934 - val_loss: 5.4840 - val_accuracy: 0.1660 - lr: 0.0500\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.0359 - accuracy: 0.4021 - val_loss: 4.9606 - val_accuracy: 0.2280 - lr: 0.0500\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.9028 - accuracy: 0.4117 - val_loss: 5.4348 - val_accuracy: 0.1505 - lr: 0.0500\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.7934 - accuracy: 0.4146 - val_loss: 5.2040 - val_accuracy: 0.1685 - lr: 0.0500\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.6799 - accuracy: 0.4206 - val_loss: 5.0790 - val_accuracy: 0.2015 - lr: 0.0500\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.5553 - accuracy: 0.4248 - val_loss: 5.2049 - val_accuracy: 0.1635 - lr: 0.0500\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.4782 - accuracy: 0.4334 - val_loss: 4.6552 - val_accuracy: 0.2210 - lr: 0.0500\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.3610 - accuracy: 0.4421 - val_loss: 4.4937 - val_accuracy: 0.2370 - lr: 0.0500\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2991 - accuracy: 0.4408 - val_loss: 4.6403 - val_accuracy: 0.2110 - lr: 0.0500\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2249 - accuracy: 0.4475 - val_loss: 4.4708 - val_accuracy: 0.2610 - lr: 0.0500\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.1360 - accuracy: 0.4491 - val_loss: 4.0491 - val_accuracy: 0.2935 - lr: 0.0500\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.0472 - accuracy: 0.4706 - val_loss: 4.1857 - val_accuracy: 0.2840 - lr: 0.0500\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9521 - accuracy: 0.4824 - val_loss: 4.2151 - val_accuracy: 0.2605 - lr: 0.0500\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8946 - accuracy: 0.4881 - val_loss: 3.8443 - val_accuracy: 0.3035 - lr: 0.0500\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8160 - accuracy: 0.4999 - val_loss: 4.0048 - val_accuracy: 0.3030 - lr: 0.0500\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7644 - accuracy: 0.5024 - val_loss: 4.1565 - val_accuracy: 0.2875 - lr: 0.0500\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7064 - accuracy: 0.5144 - val_loss: 3.4837 - val_accuracy: 0.3765 - lr: 0.0500\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.6569 - accuracy: 0.5217 - val_loss: 3.1549 - val_accuracy: 0.4235 - lr: 0.0500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4858 - accuracy: 0.5577 - val_loss: 3.3200 - val_accuracy: 0.3980 - lr: 0.0250\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3589 - accuracy: 0.5931 - val_loss: 3.4973 - val_accuracy: 0.3915 - lr: 0.0250\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3180 - accuracy: 0.6025 - val_loss: 3.2620 - val_accuracy: 0.3980 - lr: 0.0250\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2753 - accuracy: 0.6012 - val_loss: 3.5440 - val_accuracy: 0.3460 - lr: 0.0250\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1989 - accuracy: 0.6235 - val_loss: 3.5843 - val_accuracy: 0.3390 - lr: 0.0250\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1995 - accuracy: 0.6151 - val_loss: 3.1210 - val_accuracy: 0.4410 - lr: 0.0250\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1493 - accuracy: 0.6324 - val_loss: 3.0923 - val_accuracy: 0.4360 - lr: 0.0250\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1070 - accuracy: 0.6274 - val_loss: 3.0271 - val_accuracy: 0.4445 - lr: 0.0250\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0896 - accuracy: 0.6414 - val_loss: 3.3378 - val_accuracy: 0.4065 - lr: 0.0250\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0537 - accuracy: 0.6449 - val_loss: 3.1155 - val_accuracy: 0.4520 - lr: 0.0250\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0187 - accuracy: 0.6553 - val_loss: 2.9011 - val_accuracy: 0.4825 - lr: 0.0250\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0037 - accuracy: 0.6549 - val_loss: 2.9405 - val_accuracy: 0.4725 - lr: 0.0250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9543 - accuracy: 0.6666 - val_loss: 3.0873 - val_accuracy: 0.4580 - lr: 0.0250\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9316 - accuracy: 0.6733 - val_loss: 3.2394 - val_accuracy: 0.4310 - lr: 0.0250\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9310 - accuracy: 0.6704 - val_loss: 3.3275 - val_accuracy: 0.4305 - lr: 0.0250\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9155 - accuracy: 0.6674 - val_loss: 2.9416 - val_accuracy: 0.4780 - lr: 0.0250\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9101 - accuracy: 0.6727 - val_loss: 3.1302 - val_accuracy: 0.4505 - lr: 0.0250\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8350 - accuracy: 0.6975 - val_loss: 2.9316 - val_accuracy: 0.4810 - lr: 0.0250\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8444 - accuracy: 0.6946 - val_loss: 2.9356 - val_accuracy: 0.5000 - lr: 0.0250\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8221 - accuracy: 0.7001 - val_loss: 3.3518 - val_accuracy: 0.4315 - lr: 0.0250\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7465 - accuracy: 0.7199 - val_loss: 2.9411 - val_accuracy: 0.4930 - lr: 0.0125\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5953 - accuracy: 0.7667 - val_loss: 2.7827 - val_accuracy: 0.5280 - lr: 0.0125\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5436 - accuracy: 0.7751 - val_loss: 2.8379 - val_accuracy: 0.5175 - lr: 0.0125\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5148 - accuracy: 0.7857 - val_loss: 2.8806 - val_accuracy: 0.5215 - lr: 0.0125\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4623 - accuracy: 0.7994 - val_loss: 3.0152 - val_accuracy: 0.4975 - lr: 0.0125\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4382 - accuracy: 0.7952 - val_loss: 2.9458 - val_accuracy: 0.5165 - lr: 0.0125\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4388 - accuracy: 0.8011 - val_loss: 3.1671 - val_accuracy: 0.4960 - lr: 0.0125\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4053 - accuracy: 0.8098 - val_loss: 3.3340 - val_accuracy: 0.4690 - lr: 0.0125\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4055 - accuracy: 0.8089 - val_loss: 3.0277 - val_accuracy: 0.5060 - lr: 0.0125\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4206 - accuracy: 0.8019 - val_loss: 3.0286 - val_accuracy: 0.5175 - lr: 0.0125\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3889 - accuracy: 0.8146 - val_loss: 3.1666 - val_accuracy: 0.5095 - lr: 0.0125\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3583 - accuracy: 0.8200 - val_loss: 3.1284 - val_accuracy: 0.4990 - lr: 0.0125\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3456 - accuracy: 0.8288 - val_loss: 3.3762 - val_accuracy: 0.4700 - lr: 0.0125\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3568 - accuracy: 0.8134 - val_loss: 3.1183 - val_accuracy: 0.5060 - lr: 0.0125\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3206 - accuracy: 0.8345 - val_loss: 3.4962 - val_accuracy: 0.4770 - lr: 0.0125\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3426 - accuracy: 0.8199 - val_loss: 3.2254 - val_accuracy: 0.4920 - lr: 0.0125\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3331 - accuracy: 0.8261 - val_loss: 3.2059 - val_accuracy: 0.5000 - lr: 0.0125\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2835 - accuracy: 0.8414 - val_loss: 3.2153 - val_accuracy: 0.5145 - lr: 0.0125\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2516 - accuracy: 0.8522 - val_loss: 3.1176 - val_accuracy: 0.5120 - lr: 0.0125\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2810 - accuracy: 0.8432 - val_loss: 3.4035 - val_accuracy: 0.4730 - lr: 0.0125\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2271 - accuracy: 0.8562 - val_loss: 2.9518 - val_accuracy: 0.5455 - lr: 0.0063\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1259 - accuracy: 0.8896 - val_loss: 2.9048 - val_accuracy: 0.5530 - lr: 0.0063\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0978 - accuracy: 0.8991 - val_loss: 2.9198 - val_accuracy: 0.5585 - lr: 0.0063\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0697 - accuracy: 0.9076 - val_loss: 2.8809 - val_accuracy: 0.5665 - lr: 0.0063\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0615 - accuracy: 0.9094 - val_loss: 2.9992 - val_accuracy: 0.5380 - lr: 0.0063\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0393 - accuracy: 0.9119 - val_loss: 2.9472 - val_accuracy: 0.5565 - lr: 0.0063\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0297 - accuracy: 0.9156 - val_loss: 3.0202 - val_accuracy: 0.5530 - lr: 0.0063\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0195 - accuracy: 0.9137 - val_loss: 3.3012 - val_accuracy: 0.5385 - lr: 0.0063\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0262 - accuracy: 0.9159 - val_loss: 3.0304 - val_accuracy: 0.5485 - lr: 0.0063\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9911 - accuracy: 0.9225 - val_loss: 3.2384 - val_accuracy: 0.5390 - lr: 0.0063\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9850 - accuracy: 0.9277 - val_loss: 3.3664 - val_accuracy: 0.5180 - lr: 0.0063\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9808 - accuracy: 0.9229 - val_loss: 3.0349 - val_accuracy: 0.5560 - lr: 0.0063\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9856 - accuracy: 0.9199 - val_loss: 2.9889 - val_accuracy: 0.5735 - lr: 0.0063\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9881 - accuracy: 0.9205 - val_loss: 2.9941 - val_accuracy: 0.5700 - lr: 0.0063\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9734 - accuracy: 0.9222 - val_loss: 3.1769 - val_accuracy: 0.5465 - lr: 0.0063\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9641 - accuracy: 0.9261 - val_loss: 3.3186 - val_accuracy: 0.5300 - lr: 0.0063\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9582 - accuracy: 0.9221 - val_loss: 3.3040 - val_accuracy: 0.5370 - lr: 0.0063\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9461 - accuracy: 0.9306 - val_loss: 3.2509 - val_accuracy: 0.5455 - lr: 0.0063\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9234 - accuracy: 0.9352 - val_loss: 3.2801 - val_accuracy: 0.5370 - lr: 0.0063\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9338 - accuracy: 0.9308 - val_loss: 3.2143 - val_accuracy: 0.5580 - lr: 0.0063\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9127 - accuracy: 0.9400 - val_loss: 3.2039 - val_accuracy: 0.5520 - lr: 0.0031\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8709 - accuracy: 0.9517 - val_loss: 3.2026 - val_accuracy: 0.5465 - lr: 0.0031\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8605 - accuracy: 0.9571 - val_loss: 3.2602 - val_accuracy: 0.5555 - lr: 0.0031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8498 - accuracy: 0.9580 - val_loss: 3.1217 - val_accuracy: 0.5640 - lr: 0.0031\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8434 - accuracy: 0.9588 - val_loss: 3.1080 - val_accuracy: 0.5725 - lr: 0.0031\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8398 - accuracy: 0.9594 - val_loss: 3.1936 - val_accuracy: 0.5630 - lr: 0.0031\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8352 - accuracy: 0.9599 - val_loss: 3.1445 - val_accuracy: 0.5630 - lr: 0.0031\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8153 - accuracy: 0.9659 - val_loss: 3.2225 - val_accuracy: 0.5580 - lr: 0.0031\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8132 - accuracy: 0.9650 - val_loss: 3.2954 - val_accuracy: 0.5510 - lr: 0.0031\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8207 - accuracy: 0.9607 - val_loss: 3.2380 - val_accuracy: 0.5685 - lr: 0.0031\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7999 - accuracy: 0.9697 - val_loss: 3.2898 - val_accuracy: 0.5620 - lr: 0.0031\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8076 - accuracy: 0.9639 - val_loss: 3.2617 - val_accuracy: 0.5610 - lr: 0.0031\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8045 - accuracy: 0.9666 - val_loss: 3.2726 - val_accuracy: 0.5575 - lr: 0.0031\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7970 - accuracy: 0.9670 - val_loss: 3.2891 - val_accuracy: 0.5680 - lr: 0.0031\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7949 - accuracy: 0.9659 - val_loss: 3.3562 - val_accuracy: 0.5605 - lr: 0.0031\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7962 - accuracy: 0.9647 - val_loss: 3.2323 - val_accuracy: 0.5660 - lr: 0.0031\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7786 - accuracy: 0.9711 - val_loss: 3.2618 - val_accuracy: 0.5610 - lr: 0.0031\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7845 - accuracy: 0.9655 - val_loss: 3.2570 - val_accuracy: 0.5625 - lr: 0.0031\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7708 - accuracy: 0.9732 - val_loss: 3.2752 - val_accuracy: 0.5590 - lr: 0.0031\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7802 - accuracy: 0.9685 - val_loss: 3.3588 - val_accuracy: 0.5480 - lr: 0.0031\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7721 - accuracy: 0.9721 - val_loss: 3.2707 - val_accuracy: 0.5660 - lr: 0.0016\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7544 - accuracy: 0.9759 - val_loss: 3.2097 - val_accuracy: 0.5715 - lr: 0.0016\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7456 - accuracy: 0.9784 - val_loss: 3.2236 - val_accuracy: 0.5785 - lr: 0.0016\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7422 - accuracy: 0.9790 - val_loss: 3.2335 - val_accuracy: 0.5730 - lr: 0.0016\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7383 - accuracy: 0.9793 - val_loss: 3.2304 - val_accuracy: 0.5705 - lr: 0.0016\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7397 - accuracy: 0.9778 - val_loss: 3.2370 - val_accuracy: 0.5775 - lr: 0.0016\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7311 - accuracy: 0.9816 - val_loss: 3.2304 - val_accuracy: 0.5790 - lr: 0.0016\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7301 - accuracy: 0.9808 - val_loss: 3.2417 - val_accuracy: 0.5765 - lr: 0.0016\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7299 - accuracy: 0.9803 - val_loss: 3.2641 - val_accuracy: 0.5695 - lr: 0.0016\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7343 - accuracy: 0.9810 - val_loss: 3.2297 - val_accuracy: 0.5770 - lr: 0.0016\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7239 - accuracy: 0.9824 - val_loss: 3.2881 - val_accuracy: 0.5720 - lr: 0.0016\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7276 - accuracy: 0.9799 - val_loss: 3.2293 - val_accuracy: 0.5735 - lr: 0.0016\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7244 - accuracy: 0.9804 - val_loss: 3.2495 - val_accuracy: 0.5710 - lr: 0.0016\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7209 - accuracy: 0.9816 - val_loss: 3.2605 - val_accuracy: 0.5740 - lr: 0.0016\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7196 - accuracy: 0.9809 - val_loss: 3.2459 - val_accuracy: 0.5710 - lr: 0.0016\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7154 - accuracy: 0.9840 - val_loss: 3.2446 - val_accuracy: 0.5760 - lr: 0.0016\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7160 - accuracy: 0.9841 - val_loss: 3.2596 - val_accuracy: 0.5725 - lr: 0.0016\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7214 - accuracy: 0.9806 - val_loss: 3.2616 - val_accuracy: 0.5780 - lr: 0.0016\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7105 - accuracy: 0.9826 - val_loss: 3.2400 - val_accuracy: 0.5720 - lr: 0.0016\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7133 - accuracy: 0.9824 - val_loss: 3.2867 - val_accuracy: 0.5725 - lr: 0.0016\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7084 - accuracy: 0.9840 - val_loss: 3.2666 - val_accuracy: 0.5730 - lr: 7.8125e-04\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7035 - accuracy: 0.9852 - val_loss: 3.2924 - val_accuracy: 0.5630 - lr: 7.8125e-04\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6991 - accuracy: 0.9874 - val_loss: 3.2779 - val_accuracy: 0.5740 - lr: 7.8125e-04\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7001 - accuracy: 0.9850 - val_loss: 3.2786 - val_accuracy: 0.5760 - lr: 7.8125e-04\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6997 - accuracy: 0.9849 - val_loss: 3.2970 - val_accuracy: 0.5675 - lr: 7.8125e-04\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6961 - accuracy: 0.9870 - val_loss: 3.3278 - val_accuracy: 0.5725 - lr: 7.8125e-04\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7010 - accuracy: 0.9850 - val_loss: 3.2882 - val_accuracy: 0.5765 - lr: 7.8125e-04\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6962 - accuracy: 0.9859 - val_loss: 3.2840 - val_accuracy: 0.5765 - lr: 7.8125e-04\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6956 - accuracy: 0.9862 - val_loss: 3.2769 - val_accuracy: 0.5770 - lr: 7.8125e-04\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6937 - accuracy: 0.9869 - val_loss: 3.2672 - val_accuracy: 0.5770 - lr: 7.8125e-04\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6876 - accuracy: 0.9883 - val_loss: 3.2874 - val_accuracy: 0.5735 - lr: 7.8125e-04\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6953 - accuracy: 0.9855 - val_loss: 3.2699 - val_accuracy: 0.5715 - lr: 7.8125e-04\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6956 - accuracy: 0.9850 - val_loss: 3.2451 - val_accuracy: 0.5760 - lr: 7.8125e-04\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6867 - accuracy: 0.9872 - val_loss: 3.2693 - val_accuracy: 0.5745 - lr: 7.8125e-04\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6902 - accuracy: 0.9856 - val_loss: 3.2734 - val_accuracy: 0.5710 - lr: 7.8125e-04\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6875 - accuracy: 0.9872 - val_loss: 3.3288 - val_accuracy: 0.5695 - lr: 7.8125e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6843 - accuracy: 0.9875 - val_loss: 3.3150 - val_accuracy: 0.5665 - lr: 7.8125e-04\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6859 - accuracy: 0.9879 - val_loss: 3.3154 - val_accuracy: 0.5725 - lr: 7.8125e-04\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6846 - accuracy: 0.9861 - val_loss: 3.3136 - val_accuracy: 0.5780 - lr: 7.8125e-04\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6823 - accuracy: 0.9889 - val_loss: 3.2909 - val_accuracy: 0.5790 - lr: 7.8125e-04\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6883 - accuracy: 0.9856 - val_loss: 3.2968 - val_accuracy: 0.5730 - lr: 3.9063e-04\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6865 - accuracy: 0.9862 - val_loss: 3.2797 - val_accuracy: 0.5710 - lr: 3.9063e-04\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6854 - accuracy: 0.9852 - val_loss: 3.2813 - val_accuracy: 0.5720 - lr: 3.9063e-04\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6786 - accuracy: 0.9900 - val_loss: 3.2752 - val_accuracy: 0.5830 - lr: 3.9063e-04\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6789 - accuracy: 0.9890 - val_loss: 3.2920 - val_accuracy: 0.5740 - lr: 3.9063e-04\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6819 - accuracy: 0.9884 - val_loss: 3.2914 - val_accuracy: 0.5730 - lr: 3.9063e-04\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6797 - accuracy: 0.9880 - val_loss: 3.2700 - val_accuracy: 0.5800 - lr: 3.9063e-04\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6760 - accuracy: 0.9900 - val_loss: 3.2637 - val_accuracy: 0.5815 - lr: 3.9063e-04\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6808 - accuracy: 0.9889 - val_loss: 3.2817 - val_accuracy: 0.5755 - lr: 3.9063e-04\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6827 - accuracy: 0.9868 - val_loss: 3.2750 - val_accuracy: 0.5740 - lr: 3.9063e-04\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6784 - accuracy: 0.9889 - val_loss: 3.2865 - val_accuracy: 0.5765 - lr: 3.9063e-04\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6798 - accuracy: 0.9870 - val_loss: 3.2866 - val_accuracy: 0.5770 - lr: 3.9063e-04\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6752 - accuracy: 0.9887 - val_loss: 3.2783 - val_accuracy: 0.5810 - lr: 3.9063e-04\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6840 - accuracy: 0.9884 - val_loss: 3.2855 - val_accuracy: 0.5810 - lr: 3.9063e-04\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6737 - accuracy: 0.9891 - val_loss: 3.2688 - val_accuracy: 0.5790 - lr: 3.9063e-04\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6693 - accuracy: 0.9909 - val_loss: 3.2751 - val_accuracy: 0.5785 - lr: 3.9063e-04\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6820 - accuracy: 0.9859 - val_loss: 3.2780 - val_accuracy: 0.5815 - lr: 3.9063e-04\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6740 - accuracy: 0.9891 - val_loss: 3.2819 - val_accuracy: 0.5850 - lr: 3.9063e-04\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6715 - accuracy: 0.9890 - val_loss: 3.2943 - val_accuracy: 0.5795 - lr: 3.9063e-04\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6708 - accuracy: 0.9901 - val_loss: 3.2587 - val_accuracy: 0.5760 - lr: 3.9063e-04\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6748 - accuracy: 0.9877 - val_loss: 3.2742 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6757 - accuracy: 0.9875 - val_loss: 3.2694 - val_accuracy: 0.5795 - lr: 1.9531e-04\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6763 - accuracy: 0.9877 - val_loss: 3.2657 - val_accuracy: 0.5830 - lr: 1.9531e-04\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6752 - accuracy: 0.9876 - val_loss: 3.2714 - val_accuracy: 0.5800 - lr: 1.9531e-04\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6754 - accuracy: 0.9876 - val_loss: 3.2835 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6717 - accuracy: 0.9900 - val_loss: 3.2906 - val_accuracy: 0.5760 - lr: 1.9531e-04\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6713 - accuracy: 0.9889 - val_loss: 3.2955 - val_accuracy: 0.5735 - lr: 1.9531e-04\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6728 - accuracy: 0.9889 - val_loss: 3.2992 - val_accuracy: 0.5765 - lr: 1.9531e-04\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6705 - accuracy: 0.9881 - val_loss: 3.2979 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6718 - accuracy: 0.9890 - val_loss: 3.2959 - val_accuracy: 0.5765 - lr: 1.9531e-04\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6730 - accuracy: 0.9876 - val_loss: 3.2791 - val_accuracy: 0.5775 - lr: 1.9531e-04\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6663 - accuracy: 0.9905 - val_loss: 3.2866 - val_accuracy: 0.5735 - lr: 1.9531e-04\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6701 - accuracy: 0.9886 - val_loss: 3.2855 - val_accuracy: 0.5760 - lr: 1.9531e-04\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6717 - accuracy: 0.9889 - val_loss: 3.2959 - val_accuracy: 0.5775 - lr: 1.9531e-04\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6703 - accuracy: 0.9890 - val_loss: 3.2963 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6653 - accuracy: 0.9911 - val_loss: 3.2981 - val_accuracy: 0.5765 - lr: 1.9531e-04\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6627 - accuracy: 0.9921 - val_loss: 3.2982 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6660 - accuracy: 0.9902 - val_loss: 3.2963 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6651 - accuracy: 0.9911 - val_loss: 3.2892 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6687 - accuracy: 0.9899 - val_loss: 3.2894 - val_accuracy: 0.5790 - lr: 1.9531e-04\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2894 - accuracy: 0.5790\n",
      "Score for fold 1: loss of 3.2893881797790527; accuracy of 57.89999961853027%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 31ms/step - loss: 23.8132 - accuracy: 0.0342 - val_loss: 8355.1338 - val_accuracy: 0.0185 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 26.4630 - accuracy: 0.0471 - val_loss: 27.2812 - val_accuracy: 0.0075 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 23.9182 - accuracy: 0.0544 - val_loss: 22.4985 - val_accuracy: 0.0365 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 21.1646 - accuracy: 0.0796 - val_loss: 19.9927 - val_accuracy: 0.0605 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 18.8240 - accuracy: 0.0948 - val_loss: 18.3725 - val_accuracy: 0.0550 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 16.8997 - accuracy: 0.1063 - val_loss: 16.6528 - val_accuracy: 0.0380 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 15.1996 - accuracy: 0.1203 - val_loss: 15.6128 - val_accuracy: 0.0240 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 13.7493 - accuracy: 0.1314 - val_loss: 13.9355 - val_accuracy: 0.0290 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 12.4614 - accuracy: 0.1421 - val_loss: 12.7225 - val_accuracy: 0.0370 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 11.2771 - accuracy: 0.1710 - val_loss: 11.8493 - val_accuracy: 0.0155 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 10.2739 - accuracy: 0.1790 - val_loss: 11.0029 - val_accuracy: 0.0375 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 9.3143 - accuracy: 0.2052 - val_loss: 10.0148 - val_accuracy: 0.0635 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 8.4796 - accuracy: 0.2323 - val_loss: 8.9914 - val_accuracy: 0.0780 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.7659 - accuracy: 0.2397 - val_loss: 8.5517 - val_accuracy: 0.0645 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.1834 - accuracy: 0.2449 - val_loss: 7.7983 - val_accuracy: 0.0915 - lr: 0.1000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.6052 - accuracy: 0.2681 - val_loss: 7.1240 - val_accuracy: 0.1105 - lr: 0.1000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.1315 - accuracy: 0.2777 - val_loss: 6.5855 - val_accuracy: 0.1400 - lr: 0.1000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.7040 - accuracy: 0.2864 - val_loss: 6.9797 - val_accuracy: 0.0830 - lr: 0.1000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.3240 - accuracy: 0.2931 - val_loss: 6.0771 - val_accuracy: 0.1170 - lr: 0.1000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.0239 - accuracy: 0.2988 - val_loss: 5.8468 - val_accuracy: 0.1390 - lr: 0.1000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.6648 - accuracy: 0.3329 - val_loss: 5.6023 - val_accuracy: 0.1590 - lr: 0.0500\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.4150 - accuracy: 0.3609 - val_loss: 5.5525 - val_accuracy: 0.1440 - lr: 0.0500\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.2638 - accuracy: 0.3761 - val_loss: 4.9314 - val_accuracy: 0.2230 - lr: 0.0500\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.1260 - accuracy: 0.3820 - val_loss: 4.6752 - val_accuracy: 0.2495 - lr: 0.0500\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.0003 - accuracy: 0.3853 - val_loss: 5.0839 - val_accuracy: 0.1800 - lr: 0.0500\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.8626 - accuracy: 0.3904 - val_loss: 4.4818 - val_accuracy: 0.2645 - lr: 0.0500\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.7555 - accuracy: 0.3976 - val_loss: 4.4053 - val_accuracy: 0.2530 - lr: 0.0500\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.6447 - accuracy: 0.4109 - val_loss: 4.7088 - val_accuracy: 0.2090 - lr: 0.0500\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.5283 - accuracy: 0.4248 - val_loss: 4.0260 - val_accuracy: 0.2950 - lr: 0.0500\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.4284 - accuracy: 0.4326 - val_loss: 4.6940 - val_accuracy: 0.2250 - lr: 0.0500\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.3469 - accuracy: 0.4374 - val_loss: 3.9712 - val_accuracy: 0.2945 - lr: 0.0500\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2410 - accuracy: 0.4444 - val_loss: 4.1004 - val_accuracy: 0.2770 - lr: 0.0500\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.1474 - accuracy: 0.4581 - val_loss: 4.2093 - val_accuracy: 0.2540 - lr: 0.0500\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 3.0949 - accuracy: 0.4571 - val_loss: 3.9299 - val_accuracy: 0.3025 - lr: 0.0500\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9799 - accuracy: 0.4751 - val_loss: 3.6445 - val_accuracy: 0.3345 - lr: 0.0500\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9594 - accuracy: 0.4825 - val_loss: 3.5247 - val_accuracy: 0.3690 - lr: 0.0500\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8817 - accuracy: 0.4879 - val_loss: 3.5084 - val_accuracy: 0.3435 - lr: 0.0500\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8060 - accuracy: 0.4970 - val_loss: 3.4528 - val_accuracy: 0.3640 - lr: 0.0500\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7585 - accuracy: 0.5041 - val_loss: 3.6206 - val_accuracy: 0.3330 - lr: 0.0500\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7050 - accuracy: 0.5096 - val_loss: 3.5133 - val_accuracy: 0.3590 - lr: 0.0500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4927 - accuracy: 0.5612 - val_loss: 3.3460 - val_accuracy: 0.3735 - lr: 0.0250\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3936 - accuracy: 0.5897 - val_loss: 3.3052 - val_accuracy: 0.3960 - lr: 0.0250\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3313 - accuracy: 0.5932 - val_loss: 3.2140 - val_accuracy: 0.3920 - lr: 0.0250\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2768 - accuracy: 0.6087 - val_loss: 3.2877 - val_accuracy: 0.3925 - lr: 0.0250\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2398 - accuracy: 0.6140 - val_loss: 3.4279 - val_accuracy: 0.3755 - lr: 0.0250\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1982 - accuracy: 0.6166 - val_loss: 4.2982 - val_accuracy: 0.2835 - lr: 0.0250\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2060 - accuracy: 0.6087 - val_loss: 3.3053 - val_accuracy: 0.4015 - lr: 0.0250\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1278 - accuracy: 0.6313 - val_loss: 3.2817 - val_accuracy: 0.3880 - lr: 0.0250\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1388 - accuracy: 0.6255 - val_loss: 3.0974 - val_accuracy: 0.4260 - lr: 0.0250\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1155 - accuracy: 0.6304 - val_loss: 3.4636 - val_accuracy: 0.3705 - lr: 0.0250\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0275 - accuracy: 0.6522 - val_loss: 3.0388 - val_accuracy: 0.4505 - lr: 0.0250\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0143 - accuracy: 0.6499 - val_loss: 3.5606 - val_accuracy: 0.3865 - lr: 0.0250\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9785 - accuracy: 0.6684 - val_loss: 3.2794 - val_accuracy: 0.4095 - lr: 0.0250\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0006 - accuracy: 0.6566 - val_loss: 3.0642 - val_accuracy: 0.4470 - lr: 0.0250\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9615 - accuracy: 0.6766 - val_loss: 3.1188 - val_accuracy: 0.4450 - lr: 0.0250\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9325 - accuracy: 0.6721 - val_loss: 3.3209 - val_accuracy: 0.4315 - lr: 0.0250\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8792 - accuracy: 0.6873 - val_loss: 2.9271 - val_accuracy: 0.4800 - lr: 0.0250\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8704 - accuracy: 0.6931 - val_loss: 3.1303 - val_accuracy: 0.4515 - lr: 0.0250\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8623 - accuracy: 0.6905 - val_loss: 3.6650 - val_accuracy: 0.3930 - lr: 0.0250\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8301 - accuracy: 0.7041 - val_loss: 3.2321 - val_accuracy: 0.4560 - lr: 0.0250\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7244 - accuracy: 0.7364 - val_loss: 2.9302 - val_accuracy: 0.4965 - lr: 0.0125\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5815 - accuracy: 0.7754 - val_loss: 3.0127 - val_accuracy: 0.4880 - lr: 0.0125\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5531 - accuracy: 0.7763 - val_loss: 3.1525 - val_accuracy: 0.4775 - lr: 0.0125\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5090 - accuracy: 0.7879 - val_loss: 3.0586 - val_accuracy: 0.4805 - lr: 0.0125\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4663 - accuracy: 0.8074 - val_loss: 3.1909 - val_accuracy: 0.4860 - lr: 0.0125\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4564 - accuracy: 0.8061 - val_loss: 3.1662 - val_accuracy: 0.4895 - lr: 0.0125\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4498 - accuracy: 0.8117 - val_loss: 3.0841 - val_accuracy: 0.4975 - lr: 0.0125\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4275 - accuracy: 0.8081 - val_loss: 3.1402 - val_accuracy: 0.4865 - lr: 0.0125\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4447 - accuracy: 0.8076 - val_loss: 3.1877 - val_accuracy: 0.5000 - lr: 0.0125\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4000 - accuracy: 0.8213 - val_loss: 3.1877 - val_accuracy: 0.4820 - lr: 0.0125\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3924 - accuracy: 0.8215 - val_loss: 2.9417 - val_accuracy: 0.5450 - lr: 0.0125\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3635 - accuracy: 0.8286 - val_loss: 3.0629 - val_accuracy: 0.5165 - lr: 0.0125\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3718 - accuracy: 0.8253 - val_loss: 3.1810 - val_accuracy: 0.5020 - lr: 0.0125\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3704 - accuracy: 0.8240 - val_loss: 3.2625 - val_accuracy: 0.4920 - lr: 0.0125\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3188 - accuracy: 0.8414 - val_loss: 3.4023 - val_accuracy: 0.4710 - lr: 0.0125\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3485 - accuracy: 0.8266 - val_loss: 3.2715 - val_accuracy: 0.4750 - lr: 0.0125\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3433 - accuracy: 0.8349 - val_loss: 3.2856 - val_accuracy: 0.4850 - lr: 0.0125\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3239 - accuracy: 0.8395 - val_loss: 3.2108 - val_accuracy: 0.5055 - lr: 0.0125\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2938 - accuracy: 0.8495 - val_loss: 3.4279 - val_accuracy: 0.4930 - lr: 0.0125\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2812 - accuracy: 0.8482 - val_loss: 3.4723 - val_accuracy: 0.4710 - lr: 0.0125\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2182 - accuracy: 0.8704 - val_loss: 3.2765 - val_accuracy: 0.5025 - lr: 0.0063\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1327 - accuracy: 0.8978 - val_loss: 3.0866 - val_accuracy: 0.5320 - lr: 0.0063\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0985 - accuracy: 0.9053 - val_loss: 3.1758 - val_accuracy: 0.5110 - lr: 0.0063\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0768 - accuracy: 0.9133 - val_loss: 2.9902 - val_accuracy: 0.5465 - lr: 0.0063\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0534 - accuracy: 0.9185 - val_loss: 3.1131 - val_accuracy: 0.5395 - lr: 0.0063\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0591 - accuracy: 0.9134 - val_loss: 3.1815 - val_accuracy: 0.5330 - lr: 0.0063\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0187 - accuracy: 0.9266 - val_loss: 3.2839 - val_accuracy: 0.5215 - lr: 0.0063\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0345 - accuracy: 0.9219 - val_loss: 3.3084 - val_accuracy: 0.5255 - lr: 0.0063\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0052 - accuracy: 0.9290 - val_loss: 3.2337 - val_accuracy: 0.5320 - lr: 0.0063\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0120 - accuracy: 0.9252 - val_loss: 3.6168 - val_accuracy: 0.4885 - lr: 0.0063\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9930 - accuracy: 0.9296 - val_loss: 3.2931 - val_accuracy: 0.5300 - lr: 0.0063\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9852 - accuracy: 0.9316 - val_loss: 3.3450 - val_accuracy: 0.5280 - lr: 0.0063\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9875 - accuracy: 0.9289 - val_loss: 3.3158 - val_accuracy: 0.5445 - lr: 0.0063\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9693 - accuracy: 0.9342 - val_loss: 3.4072 - val_accuracy: 0.5125 - lr: 0.0063\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9637 - accuracy: 0.9358 - val_loss: 3.4550 - val_accuracy: 0.5215 - lr: 0.0063\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9723 - accuracy: 0.9342 - val_loss: 3.3018 - val_accuracy: 0.5340 - lr: 0.0063\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9484 - accuracy: 0.9365 - val_loss: 3.3777 - val_accuracy: 0.5345 - lr: 0.0063\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9584 - accuracy: 0.9340 - val_loss: 3.3515 - val_accuracy: 0.5390 - lr: 0.0063\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9524 - accuracy: 0.9356 - val_loss: 3.3834 - val_accuracy: 0.5255 - lr: 0.0063\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9438 - accuracy: 0.9349 - val_loss: 3.5182 - val_accuracy: 0.5360 - lr: 0.0063\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9205 - accuracy: 0.9450 - val_loss: 3.2079 - val_accuracy: 0.5575 - lr: 0.0031\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8652 - accuracy: 0.9615 - val_loss: 3.2682 - val_accuracy: 0.5480 - lr: 0.0031\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8772 - accuracy: 0.9544 - val_loss: 3.3217 - val_accuracy: 0.5465 - lr: 0.0031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8658 - accuracy: 0.9595 - val_loss: 3.2705 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8546 - accuracy: 0.9620 - val_loss: 3.2758 - val_accuracy: 0.5560 - lr: 0.0031\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8387 - accuracy: 0.9674 - val_loss: 3.2548 - val_accuracy: 0.5640 - lr: 0.0031\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8339 - accuracy: 0.9672 - val_loss: 3.2676 - val_accuracy: 0.5545 - lr: 0.0031\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8421 - accuracy: 0.9644 - val_loss: 3.4606 - val_accuracy: 0.5365 - lr: 0.0031\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8300 - accuracy: 0.9681 - val_loss: 3.3116 - val_accuracy: 0.5600 - lr: 0.0031\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8303 - accuracy: 0.9654 - val_loss: 3.4840 - val_accuracy: 0.5375 - lr: 0.0031\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8244 - accuracy: 0.9671 - val_loss: 3.4174 - val_accuracy: 0.5525 - lr: 0.0031\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8162 - accuracy: 0.9684 - val_loss: 3.5307 - val_accuracy: 0.5275 - lr: 0.0031\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8177 - accuracy: 0.9680 - val_loss: 3.4546 - val_accuracy: 0.5410 - lr: 0.0031\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8166 - accuracy: 0.9656 - val_loss: 3.4327 - val_accuracy: 0.5555 - lr: 0.0031\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8069 - accuracy: 0.9710 - val_loss: 3.3808 - val_accuracy: 0.5465 - lr: 0.0031\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7998 - accuracy: 0.9715 - val_loss: 3.3846 - val_accuracy: 0.5465 - lr: 0.0031\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7952 - accuracy: 0.9707 - val_loss: 3.3254 - val_accuracy: 0.5580 - lr: 0.0031\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7937 - accuracy: 0.9699 - val_loss: 3.3568 - val_accuracy: 0.5590 - lr: 0.0031\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7924 - accuracy: 0.9710 - val_loss: 3.4249 - val_accuracy: 0.5550 - lr: 0.0031\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7845 - accuracy: 0.9718 - val_loss: 3.3707 - val_accuracy: 0.5550 - lr: 0.0031\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7775 - accuracy: 0.9735 - val_loss: 3.3448 - val_accuracy: 0.5640 - lr: 0.0016\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7693 - accuracy: 0.9778 - val_loss: 3.4205 - val_accuracy: 0.5520 - lr: 0.0016\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7671 - accuracy: 0.9750 - val_loss: 3.4054 - val_accuracy: 0.5665 - lr: 0.0016\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7640 - accuracy: 0.9779 - val_loss: 3.3639 - val_accuracy: 0.5625 - lr: 0.0016\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7499 - accuracy: 0.9824 - val_loss: 3.3576 - val_accuracy: 0.5620 - lr: 0.0016\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7455 - accuracy: 0.9837 - val_loss: 3.4249 - val_accuracy: 0.5600 - lr: 0.0016\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7533 - accuracy: 0.9772 - val_loss: 3.3415 - val_accuracy: 0.5685 - lr: 0.0016\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7545 - accuracy: 0.9781 - val_loss: 3.3567 - val_accuracy: 0.5745 - lr: 0.0016\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7422 - accuracy: 0.9835 - val_loss: 3.4055 - val_accuracy: 0.5615 - lr: 0.0016\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7386 - accuracy: 0.9841 - val_loss: 3.3465 - val_accuracy: 0.5720 - lr: 0.0016\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7389 - accuracy: 0.9826 - val_loss: 3.3454 - val_accuracy: 0.5685 - lr: 0.0016\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7363 - accuracy: 0.9839 - val_loss: 3.3607 - val_accuracy: 0.5675 - lr: 0.0016\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7419 - accuracy: 0.9820 - val_loss: 3.3955 - val_accuracy: 0.5615 - lr: 0.0016\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7366 - accuracy: 0.9820 - val_loss: 3.3913 - val_accuracy: 0.5590 - lr: 0.0016\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7342 - accuracy: 0.9814 - val_loss: 3.4207 - val_accuracy: 0.5580 - lr: 0.0016\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7324 - accuracy: 0.9827 - val_loss: 3.4156 - val_accuracy: 0.5595 - lr: 0.0016\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7360 - accuracy: 0.9824 - val_loss: 3.5020 - val_accuracy: 0.5505 - lr: 0.0016\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7257 - accuracy: 0.9839 - val_loss: 3.4000 - val_accuracy: 0.5580 - lr: 0.0016\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7307 - accuracy: 0.9820 - val_loss: 3.3623 - val_accuracy: 0.5615 - lr: 0.0016\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7267 - accuracy: 0.9826 - val_loss: 3.4002 - val_accuracy: 0.5745 - lr: 0.0016\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7180 - accuracy: 0.9841 - val_loss: 3.4183 - val_accuracy: 0.5630 - lr: 7.8125e-04\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7188 - accuracy: 0.9845 - val_loss: 3.4151 - val_accuracy: 0.5620 - lr: 7.8125e-04\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7187 - accuracy: 0.9834 - val_loss: 3.3896 - val_accuracy: 0.5720 - lr: 7.8125e-04\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7157 - accuracy: 0.9861 - val_loss: 3.3700 - val_accuracy: 0.5730 - lr: 7.8125e-04\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7164 - accuracy: 0.9852 - val_loss: 3.3750 - val_accuracy: 0.5700 - lr: 7.8125e-04\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7189 - accuracy: 0.9840 - val_loss: 3.3428 - val_accuracy: 0.5715 - lr: 7.8125e-04\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7070 - accuracy: 0.9865 - val_loss: 3.3996 - val_accuracy: 0.5670 - lr: 7.8125e-04\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7056 - accuracy: 0.9881 - val_loss: 3.3549 - val_accuracy: 0.5735 - lr: 7.8125e-04\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7039 - accuracy: 0.9883 - val_loss: 3.3808 - val_accuracy: 0.5675 - lr: 7.8125e-04\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7052 - accuracy: 0.9884 - val_loss: 3.3665 - val_accuracy: 0.5725 - lr: 7.8125e-04\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7069 - accuracy: 0.9864 - val_loss: 3.3598 - val_accuracy: 0.5655 - lr: 7.8125e-04\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6988 - accuracy: 0.9900 - val_loss: 3.3623 - val_accuracy: 0.5650 - lr: 7.8125e-04\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7032 - accuracy: 0.9871 - val_loss: 3.3922 - val_accuracy: 0.5645 - lr: 7.8125e-04\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7037 - accuracy: 0.9868 - val_loss: 3.4179 - val_accuracy: 0.5665 - lr: 7.8125e-04\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7056 - accuracy: 0.9872 - val_loss: 3.4210 - val_accuracy: 0.5690 - lr: 7.8125e-04\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6972 - accuracy: 0.9885 - val_loss: 3.3954 - val_accuracy: 0.5610 - lr: 7.8125e-04\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7008 - accuracy: 0.9879 - val_loss: 3.3990 - val_accuracy: 0.5665 - lr: 7.8125e-04\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7005 - accuracy: 0.9866 - val_loss: 3.4029 - val_accuracy: 0.5645 - lr: 7.8125e-04\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6967 - accuracy: 0.9880 - val_loss: 3.3945 - val_accuracy: 0.5575 - lr: 7.8125e-04\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6935 - accuracy: 0.9883 - val_loss: 3.4168 - val_accuracy: 0.5610 - lr: 7.8125e-04\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7057 - accuracy: 0.9865 - val_loss: 3.4035 - val_accuracy: 0.5665 - lr: 3.9063e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6959 - accuracy: 0.9874 - val_loss: 3.3947 - val_accuracy: 0.5685 - lr: 3.9063e-04\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6942 - accuracy: 0.9883 - val_loss: 3.3974 - val_accuracy: 0.5645 - lr: 3.9063e-04\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6929 - accuracy: 0.9895 - val_loss: 3.3997 - val_accuracy: 0.5665 - lr: 3.9063e-04\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6902 - accuracy: 0.9900 - val_loss: 3.3781 - val_accuracy: 0.5655 - lr: 3.9063e-04\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6854 - accuracy: 0.9915 - val_loss: 3.3872 - val_accuracy: 0.5640 - lr: 3.9063e-04\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6972 - accuracy: 0.9891 - val_loss: 3.3743 - val_accuracy: 0.5675 - lr: 3.9063e-04\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6921 - accuracy: 0.9896 - val_loss: 3.3741 - val_accuracy: 0.5695 - lr: 3.9063e-04\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7006 - accuracy: 0.9847 - val_loss: 3.3651 - val_accuracy: 0.5720 - lr: 3.9063e-04\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6894 - accuracy: 0.9896 - val_loss: 3.3882 - val_accuracy: 0.5685 - lr: 3.9063e-04\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6920 - accuracy: 0.9893 - val_loss: 3.4035 - val_accuracy: 0.5655 - lr: 3.9063e-04\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6903 - accuracy: 0.9908 - val_loss: 3.4031 - val_accuracy: 0.5680 - lr: 3.9063e-04\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6867 - accuracy: 0.9912 - val_loss: 3.3883 - val_accuracy: 0.5705 - lr: 3.9063e-04\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6850 - accuracy: 0.9910 - val_loss: 3.3823 - val_accuracy: 0.5725 - lr: 3.9063e-04\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6977 - accuracy: 0.9864 - val_loss: 3.3934 - val_accuracy: 0.5715 - lr: 3.9063e-04\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6938 - accuracy: 0.9883 - val_loss: 3.4141 - val_accuracy: 0.5695 - lr: 3.9063e-04\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6869 - accuracy: 0.9902 - val_loss: 3.4016 - val_accuracy: 0.5710 - lr: 3.9063e-04\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6882 - accuracy: 0.9906 - val_loss: 3.3921 - val_accuracy: 0.5665 - lr: 3.9063e-04\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6848 - accuracy: 0.9916 - val_loss: 3.3912 - val_accuracy: 0.5685 - lr: 3.9063e-04\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6897 - accuracy: 0.9889 - val_loss: 3.3819 - val_accuracy: 0.5695 - lr: 3.9063e-04\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6879 - accuracy: 0.9896 - val_loss: 3.3989 - val_accuracy: 0.5700 - lr: 1.9531e-04\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6829 - accuracy: 0.9904 - val_loss: 3.3994 - val_accuracy: 0.5680 - lr: 1.9531e-04\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6865 - accuracy: 0.9904 - val_loss: 3.3853 - val_accuracy: 0.5700 - lr: 1.9531e-04\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6790 - accuracy: 0.9916 - val_loss: 3.3871 - val_accuracy: 0.5670 - lr: 1.9531e-04\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6846 - accuracy: 0.9900 - val_loss: 3.3916 - val_accuracy: 0.5660 - lr: 1.9531e-04\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6864 - accuracy: 0.9893 - val_loss: 3.3937 - val_accuracy: 0.5675 - lr: 1.9531e-04\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6918 - accuracy: 0.9872 - val_loss: 3.3801 - val_accuracy: 0.5710 - lr: 1.9531e-04\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6838 - accuracy: 0.9909 - val_loss: 3.3878 - val_accuracy: 0.5710 - lr: 1.9531e-04\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6838 - accuracy: 0.9904 - val_loss: 3.3863 - val_accuracy: 0.5715 - lr: 1.9531e-04\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6869 - accuracy: 0.9886 - val_loss: 3.3835 - val_accuracy: 0.5725 - lr: 1.9531e-04\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6880 - accuracy: 0.9884 - val_loss: 3.3912 - val_accuracy: 0.5665 - lr: 1.9531e-04\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6801 - accuracy: 0.9919 - val_loss: 3.3890 - val_accuracy: 0.5695 - lr: 1.9531e-04\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6851 - accuracy: 0.9893 - val_loss: 3.3948 - val_accuracy: 0.5670 - lr: 1.9531e-04\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6785 - accuracy: 0.9927 - val_loss: 3.3936 - val_accuracy: 0.5680 - lr: 1.9531e-04\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6788 - accuracy: 0.9908 - val_loss: 3.3925 - val_accuracy: 0.5665 - lr: 1.9531e-04\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6819 - accuracy: 0.9902 - val_loss: 3.3755 - val_accuracy: 0.5685 - lr: 1.9531e-04\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6815 - accuracy: 0.9911 - val_loss: 3.3824 - val_accuracy: 0.5695 - lr: 1.9531e-04\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6876 - accuracy: 0.9876 - val_loss: 3.3892 - val_accuracy: 0.5695 - lr: 1.9531e-04\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6767 - accuracy: 0.9924 - val_loss: 3.3871 - val_accuracy: 0.5690 - lr: 1.9531e-04\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6805 - accuracy: 0.9909 - val_loss: 3.3841 - val_accuracy: 0.5690 - lr: 1.9531e-04\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3841 - accuracy: 0.5695\n",
      "Score for fold 2: loss of 3.384105920791626; accuracy of 56.950002908706665%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 35ms/step - loss: 23.7418 - accuracy: 0.0539 - val_loss: 19431.5098 - val_accuracy: 0.0135 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 25.6903 - accuracy: 0.0960 - val_loss: 64.7768 - val_accuracy: 0.0170 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 22.5813 - accuracy: 0.1109 - val_loss: 22.2022 - val_accuracy: 0.0620 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 20.4394 - accuracy: 0.1134 - val_loss: 20.2805 - val_accuracy: 0.0395 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 18.3085 - accuracy: 0.1279 - val_loss: 21.0913 - val_accuracy: 0.0260 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 16.4642 - accuracy: 0.1507 - val_loss: 25.5023 - val_accuracy: 0.0205 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 14.8212 - accuracy: 0.1649 - val_loss: 20.2536 - val_accuracy: 0.0360 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 13.3655 - accuracy: 0.1794 - val_loss: 15.9760 - val_accuracy: 0.0255 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 12.1126 - accuracy: 0.1970 - val_loss: 13.8808 - val_accuracy: 0.0275 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 10.8955 - accuracy: 0.2218 - val_loss: 12.9905 - val_accuracy: 0.0295 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 9.9211 - accuracy: 0.2245 - val_loss: 11.6522 - val_accuracy: 0.0395 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 8.9983 - accuracy: 0.2516 - val_loss: 10.7955 - val_accuracy: 0.0325 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 8.2042 - accuracy: 0.2694 - val_loss: 10.5811 - val_accuracy: 0.0360 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.5177 - accuracy: 0.2743 - val_loss: 9.7445 - val_accuracy: 0.0510 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.8987 - accuracy: 0.3009 - val_loss: 8.9003 - val_accuracy: 0.0615 - lr: 0.1000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.3620 - accuracy: 0.3088 - val_loss: 8.3582 - val_accuracy: 0.0615 - lr: 0.1000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.8743 - accuracy: 0.3237 - val_loss: 8.2684 - val_accuracy: 0.0590 - lr: 0.1000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.4542 - accuracy: 0.3415 - val_loss: 7.1112 - val_accuracy: 0.0880 - lr: 0.1000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.0873 - accuracy: 0.3499 - val_loss: 6.8158 - val_accuracy: 0.0885 - lr: 0.1000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.7622 - accuracy: 0.3615 - val_loss: 6.3602 - val_accuracy: 0.1285 - lr: 0.1000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.4295 - accuracy: 0.3955 - val_loss: 6.2599 - val_accuracy: 0.1290 - lr: 0.0500\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.2040 - accuracy: 0.4191 - val_loss: 6.1846 - val_accuracy: 0.1250 - lr: 0.0500\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.0476 - accuracy: 0.4277 - val_loss: 5.8479 - val_accuracy: 0.1410 - lr: 0.0500\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.8949 - accuracy: 0.4414 - val_loss: 5.7353 - val_accuracy: 0.1410 - lr: 0.0500\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.7610 - accuracy: 0.4481 - val_loss: 5.2662 - val_accuracy: 0.1995 - lr: 0.0500\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.6472 - accuracy: 0.4564 - val_loss: 4.7937 - val_accuracy: 0.2235 - lr: 0.0500\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.5236 - accuracy: 0.4748 - val_loss: 4.7185 - val_accuracy: 0.2440 - lr: 0.0500\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.4265 - accuracy: 0.4724 - val_loss: 5.3710 - val_accuracy: 0.1755 - lr: 0.0500\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.3056 - accuracy: 0.4819 - val_loss: 4.6798 - val_accuracy: 0.2255 - lr: 0.0500\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2383 - accuracy: 0.4823 - val_loss: 4.3818 - val_accuracy: 0.2275 - lr: 0.0500\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.1314 - accuracy: 0.5011 - val_loss: 3.9401 - val_accuracy: 0.3330 - lr: 0.0500\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.0739 - accuracy: 0.5056 - val_loss: 3.9909 - val_accuracy: 0.3085 - lr: 0.0500\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9959 - accuracy: 0.5034 - val_loss: 4.0457 - val_accuracy: 0.2950 - lr: 0.0500\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9022 - accuracy: 0.5232 - val_loss: 3.7032 - val_accuracy: 0.3405 - lr: 0.0500\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8393 - accuracy: 0.5229 - val_loss: 3.9333 - val_accuracy: 0.3000 - lr: 0.0500\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7817 - accuracy: 0.5206 - val_loss: 3.8270 - val_accuracy: 0.3245 - lr: 0.0500\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7156 - accuracy: 0.5353 - val_loss: 3.6502 - val_accuracy: 0.3620 - lr: 0.0500\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.6615 - accuracy: 0.5468 - val_loss: 3.6356 - val_accuracy: 0.3630 - lr: 0.0500\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.6057 - accuracy: 0.5460 - val_loss: 3.3732 - val_accuracy: 0.3925 - lr: 0.0500\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.5701 - accuracy: 0.5559 - val_loss: 3.8631 - val_accuracy: 0.3265 - lr: 0.0500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4223 - accuracy: 0.5895 - val_loss: 3.5189 - val_accuracy: 0.3690 - lr: 0.0250\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2718 - accuracy: 0.6217 - val_loss: 3.2857 - val_accuracy: 0.4220 - lr: 0.0250\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2299 - accuracy: 0.6309 - val_loss: 3.4213 - val_accuracy: 0.3865 - lr: 0.0250\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1936 - accuracy: 0.6391 - val_loss: 3.5160 - val_accuracy: 0.3800 - lr: 0.0250\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1299 - accuracy: 0.6496 - val_loss: 3.5928 - val_accuracy: 0.3725 - lr: 0.0250\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1107 - accuracy: 0.6550 - val_loss: 3.6498 - val_accuracy: 0.3740 - lr: 0.0250\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0716 - accuracy: 0.6611 - val_loss: 3.2540 - val_accuracy: 0.4195 - lr: 0.0250\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0342 - accuracy: 0.6711 - val_loss: 3.0207 - val_accuracy: 0.4700 - lr: 0.0250\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0275 - accuracy: 0.6714 - val_loss: 3.1046 - val_accuracy: 0.4355 - lr: 0.0250\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9944 - accuracy: 0.6734 - val_loss: 3.1422 - val_accuracy: 0.4390 - lr: 0.0250\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9516 - accuracy: 0.6790 - val_loss: 3.1803 - val_accuracy: 0.4300 - lr: 0.0250\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9381 - accuracy: 0.6791 - val_loss: 3.2552 - val_accuracy: 0.4380 - lr: 0.0250\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8894 - accuracy: 0.6956 - val_loss: 3.2316 - val_accuracy: 0.4410 - lr: 0.0250\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8883 - accuracy: 0.6936 - val_loss: 3.4394 - val_accuracy: 0.4140 - lr: 0.0250\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8550 - accuracy: 0.7016 - val_loss: 3.5988 - val_accuracy: 0.3925 - lr: 0.0250\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8324 - accuracy: 0.7045 - val_loss: 3.0287 - val_accuracy: 0.4820 - lr: 0.0250\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7924 - accuracy: 0.7138 - val_loss: 3.1070 - val_accuracy: 0.4555 - lr: 0.0250\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8087 - accuracy: 0.7163 - val_loss: 2.9983 - val_accuracy: 0.4785 - lr: 0.0250\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7830 - accuracy: 0.7155 - val_loss: 3.4641 - val_accuracy: 0.4105 - lr: 0.0250\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7500 - accuracy: 0.7334 - val_loss: 2.9248 - val_accuracy: 0.5010 - lr: 0.0250\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.6702 - accuracy: 0.7519 - val_loss: 2.8274 - val_accuracy: 0.5200 - lr: 0.0125\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.5069 - accuracy: 0.8041 - val_loss: 3.0474 - val_accuracy: 0.4905 - lr: 0.0125\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 27ms/step - loss: 1.4857 - accuracy: 0.8061 - val_loss: 3.2064 - val_accuracy: 0.4660 - lr: 0.0125\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4538 - accuracy: 0.8130 - val_loss: 3.1265 - val_accuracy: 0.4885 - lr: 0.0125\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4239 - accuracy: 0.8204 - val_loss: 3.0740 - val_accuracy: 0.5100 - lr: 0.0125\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4308 - accuracy: 0.8179 - val_loss: 3.1884 - val_accuracy: 0.4745 - lr: 0.0125\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3896 - accuracy: 0.8305 - val_loss: 3.0862 - val_accuracy: 0.5100 - lr: 0.0125\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3701 - accuracy: 0.8322 - val_loss: 3.0753 - val_accuracy: 0.5275 - lr: 0.0125\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3506 - accuracy: 0.8382 - val_loss: 3.2992 - val_accuracy: 0.4890 - lr: 0.0125\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3363 - accuracy: 0.8429 - val_loss: 3.0052 - val_accuracy: 0.5260 - lr: 0.0125\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3185 - accuracy: 0.8451 - val_loss: 3.2314 - val_accuracy: 0.5030 - lr: 0.0125\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3386 - accuracy: 0.8406 - val_loss: 3.1489 - val_accuracy: 0.5155 - lr: 0.0125\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2895 - accuracy: 0.8531 - val_loss: 3.0804 - val_accuracy: 0.5065 - lr: 0.0125\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2742 - accuracy: 0.8574 - val_loss: 3.1540 - val_accuracy: 0.5170 - lr: 0.0125\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2708 - accuracy: 0.8609 - val_loss: 3.1550 - val_accuracy: 0.5270 - lr: 0.0125\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2730 - accuracy: 0.8568 - val_loss: 3.1113 - val_accuracy: 0.5395 - lr: 0.0125\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2443 - accuracy: 0.8645 - val_loss: 3.3880 - val_accuracy: 0.4980 - lr: 0.0125\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2383 - accuracy: 0.8666 - val_loss: 3.5078 - val_accuracy: 0.4800 - lr: 0.0125\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2431 - accuracy: 0.8641 - val_loss: 3.3106 - val_accuracy: 0.4910 - lr: 0.0125\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2365 - accuracy: 0.8656 - val_loss: 3.3584 - val_accuracy: 0.5065 - lr: 0.0125\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1901 - accuracy: 0.8810 - val_loss: 2.9560 - val_accuracy: 0.5630 - lr: 0.0063\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0964 - accuracy: 0.9101 - val_loss: 3.1567 - val_accuracy: 0.5335 - lr: 0.0063\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0774 - accuracy: 0.9131 - val_loss: 2.9754 - val_accuracy: 0.5540 - lr: 0.0063\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0495 - accuracy: 0.9247 - val_loss: 3.0684 - val_accuracy: 0.5545 - lr: 0.0063\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0266 - accuracy: 0.9277 - val_loss: 3.0178 - val_accuracy: 0.5535 - lr: 0.0063\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0078 - accuracy: 0.9339 - val_loss: 3.1827 - val_accuracy: 0.5325 - lr: 0.0063\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0009 - accuracy: 0.9342 - val_loss: 3.1784 - val_accuracy: 0.5405 - lr: 0.0063\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0079 - accuracy: 0.9296 - val_loss: 3.1680 - val_accuracy: 0.5400 - lr: 0.0063\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0026 - accuracy: 0.9312 - val_loss: 3.2390 - val_accuracy: 0.5455 - lr: 0.0063\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9744 - accuracy: 0.9386 - val_loss: 3.3036 - val_accuracy: 0.5475 - lr: 0.0063\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9771 - accuracy: 0.9370 - val_loss: 3.2085 - val_accuracy: 0.5490 - lr: 0.0063\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9745 - accuracy: 0.9370 - val_loss: 3.2430 - val_accuracy: 0.5380 - lr: 0.0063\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9410 - accuracy: 0.9442 - val_loss: 3.2341 - val_accuracy: 0.5415 - lr: 0.0063\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9489 - accuracy: 0.9395 - val_loss: 3.2250 - val_accuracy: 0.5465 - lr: 0.0063\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.9408 - accuracy: 0.9435 - val_loss: 3.4548 - val_accuracy: 0.5260 - lr: 0.0063\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9349 - accuracy: 0.9434 - val_loss: 3.2809 - val_accuracy: 0.5565 - lr: 0.0063\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9289 - accuracy: 0.9457 - val_loss: 3.2364 - val_accuracy: 0.5565 - lr: 0.0063\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9304 - accuracy: 0.9416 - val_loss: 3.4195 - val_accuracy: 0.5335 - lr: 0.0063\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9240 - accuracy: 0.9440 - val_loss: 3.2561 - val_accuracy: 0.5510 - lr: 0.0063\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9233 - accuracy: 0.9429 - val_loss: 3.1950 - val_accuracy: 0.5660 - lr: 0.0063\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8968 - accuracy: 0.9515 - val_loss: 3.3482 - val_accuracy: 0.5495 - lr: 0.0031\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8565 - accuracy: 0.9631 - val_loss: 3.2054 - val_accuracy: 0.5685 - lr: 0.0031\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8538 - accuracy: 0.9656 - val_loss: 3.1456 - val_accuracy: 0.5775 - lr: 0.0031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8534 - accuracy: 0.9613 - val_loss: 3.1183 - val_accuracy: 0.5735 - lr: 0.0031\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8428 - accuracy: 0.9664 - val_loss: 3.2571 - val_accuracy: 0.5630 - lr: 0.0031\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8358 - accuracy: 0.9672 - val_loss: 3.1670 - val_accuracy: 0.5730 - lr: 0.0031\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8229 - accuracy: 0.9705 - val_loss: 3.1963 - val_accuracy: 0.5770 - lr: 0.0031\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8126 - accuracy: 0.9730 - val_loss: 3.1963 - val_accuracy: 0.5735 - lr: 0.0031\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8082 - accuracy: 0.9735 - val_loss: 3.1953 - val_accuracy: 0.5815 - lr: 0.0031\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8105 - accuracy: 0.9706 - val_loss: 3.1604 - val_accuracy: 0.5885 - lr: 0.0031\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7991 - accuracy: 0.9755 - val_loss: 3.2404 - val_accuracy: 0.5765 - lr: 0.0031\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8000 - accuracy: 0.9736 - val_loss: 3.2674 - val_accuracy: 0.5665 - lr: 0.0031\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7935 - accuracy: 0.9758 - val_loss: 3.1897 - val_accuracy: 0.5800 - lr: 0.0031\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7929 - accuracy: 0.9754 - val_loss: 3.2460 - val_accuracy: 0.5750 - lr: 0.0031\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7949 - accuracy: 0.9746 - val_loss: 3.2453 - val_accuracy: 0.5750 - lr: 0.0031\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8004 - accuracy: 0.9728 - val_loss: 3.3024 - val_accuracy: 0.5700 - lr: 0.0031\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7917 - accuracy: 0.9710 - val_loss: 3.3259 - val_accuracy: 0.5660 - lr: 0.0031\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7817 - accuracy: 0.9734 - val_loss: 3.3250 - val_accuracy: 0.5680 - lr: 0.0031\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7821 - accuracy: 0.9731 - val_loss: 3.2563 - val_accuracy: 0.5800 - lr: 0.0031\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7849 - accuracy: 0.9737 - val_loss: 3.2289 - val_accuracy: 0.5790 - lr: 0.0031\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7705 - accuracy: 0.9770 - val_loss: 3.2426 - val_accuracy: 0.5730 - lr: 0.0016\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7604 - accuracy: 0.9793 - val_loss: 3.2608 - val_accuracy: 0.5715 - lr: 0.0016\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7508 - accuracy: 0.9837 - val_loss: 3.2563 - val_accuracy: 0.5790 - lr: 0.0016\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7505 - accuracy: 0.9818 - val_loss: 3.2274 - val_accuracy: 0.5870 - lr: 0.0016\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7486 - accuracy: 0.9822 - val_loss: 3.2139 - val_accuracy: 0.5925 - lr: 0.0016\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7478 - accuracy: 0.9827 - val_loss: 3.2356 - val_accuracy: 0.5890 - lr: 0.0016\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7438 - accuracy: 0.9844 - val_loss: 3.1779 - val_accuracy: 0.5885 - lr: 0.0016\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7389 - accuracy: 0.9851 - val_loss: 3.2453 - val_accuracy: 0.5850 - lr: 0.0016\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7360 - accuracy: 0.9836 - val_loss: 3.2579 - val_accuracy: 0.5755 - lr: 0.0016\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7350 - accuracy: 0.9841 - val_loss: 3.2619 - val_accuracy: 0.5820 - lr: 0.0016\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7340 - accuracy: 0.9861 - val_loss: 3.2750 - val_accuracy: 0.5850 - lr: 0.0016\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7396 - accuracy: 0.9815 - val_loss: 3.1744 - val_accuracy: 0.5915 - lr: 0.0016\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7329 - accuracy: 0.9844 - val_loss: 3.2221 - val_accuracy: 0.5835 - lr: 0.0016\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7296 - accuracy: 0.9843 - val_loss: 3.2839 - val_accuracy: 0.5820 - lr: 0.0016\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7239 - accuracy: 0.9855 - val_loss: 3.2595 - val_accuracy: 0.5805 - lr: 0.0016\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7218 - accuracy: 0.9872 - val_loss: 3.2875 - val_accuracy: 0.5755 - lr: 0.0016\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7201 - accuracy: 0.9864 - val_loss: 3.2924 - val_accuracy: 0.5830 - lr: 0.0016\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7241 - accuracy: 0.9843 - val_loss: 3.2741 - val_accuracy: 0.5870 - lr: 0.0016\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7169 - accuracy: 0.9870 - val_loss: 3.2947 - val_accuracy: 0.5880 - lr: 0.0016\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7169 - accuracy: 0.9869 - val_loss: 3.2756 - val_accuracy: 0.5875 - lr: 0.0016\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7160 - accuracy: 0.9856 - val_loss: 3.2775 - val_accuracy: 0.5885 - lr: 7.8125e-04\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7154 - accuracy: 0.9868 - val_loss: 3.2668 - val_accuracy: 0.5845 - lr: 7.8125e-04\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7140 - accuracy: 0.9862 - val_loss: 3.2765 - val_accuracy: 0.5855 - lr: 7.8125e-04\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7116 - accuracy: 0.9874 - val_loss: 3.2723 - val_accuracy: 0.5860 - lr: 7.8125e-04\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7091 - accuracy: 0.9880 - val_loss: 3.2763 - val_accuracy: 0.5900 - lr: 7.8125e-04\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7156 - accuracy: 0.9866 - val_loss: 3.3056 - val_accuracy: 0.5875 - lr: 7.8125e-04\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7119 - accuracy: 0.9870 - val_loss: 3.3102 - val_accuracy: 0.5935 - lr: 7.8125e-04\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7059 - accuracy: 0.9877 - val_loss: 3.2929 - val_accuracy: 0.5865 - lr: 7.8125e-04\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7047 - accuracy: 0.9883 - val_loss: 3.2918 - val_accuracy: 0.5850 - lr: 7.8125e-04\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7061 - accuracy: 0.9870 - val_loss: 3.2911 - val_accuracy: 0.5855 - lr: 7.8125e-04\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6986 - accuracy: 0.9904 - val_loss: 3.2625 - val_accuracy: 0.5940 - lr: 7.8125e-04\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7023 - accuracy: 0.9876 - val_loss: 3.2598 - val_accuracy: 0.5880 - lr: 7.8125e-04\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6969 - accuracy: 0.9898 - val_loss: 3.2748 - val_accuracy: 0.5880 - lr: 7.8125e-04\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7027 - accuracy: 0.9869 - val_loss: 3.2701 - val_accuracy: 0.5895 - lr: 7.8125e-04\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7000 - accuracy: 0.9889 - val_loss: 3.2830 - val_accuracy: 0.5900 - lr: 7.8125e-04\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7059 - accuracy: 0.9865 - val_loss: 3.2771 - val_accuracy: 0.5905 - lr: 7.8125e-04\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7033 - accuracy: 0.9869 - val_loss: 3.2998 - val_accuracy: 0.5890 - lr: 7.8125e-04\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6993 - accuracy: 0.9884 - val_loss: 3.2700 - val_accuracy: 0.5925 - lr: 7.8125e-04\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6952 - accuracy: 0.9887 - val_loss: 3.2721 - val_accuracy: 0.5940 - lr: 7.8125e-04\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6914 - accuracy: 0.9906 - val_loss: 3.2819 - val_accuracy: 0.5860 - lr: 7.8125e-04\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6914 - accuracy: 0.9905 - val_loss: 3.2743 - val_accuracy: 0.5890 - lr: 3.9063e-04\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6951 - accuracy: 0.9899 - val_loss: 3.2722 - val_accuracy: 0.5895 - lr: 3.9063e-04\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6899 - accuracy: 0.9904 - val_loss: 3.2794 - val_accuracy: 0.5885 - lr: 3.9063e-04\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6917 - accuracy: 0.9890 - val_loss: 3.2932 - val_accuracy: 0.5895 - lr: 3.9063e-04\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6949 - accuracy: 0.9898 - val_loss: 3.2914 - val_accuracy: 0.5935 - lr: 3.9063e-04\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6865 - accuracy: 0.9908 - val_loss: 3.2817 - val_accuracy: 0.5915 - lr: 3.9063e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6908 - accuracy: 0.9894 - val_loss: 3.2865 - val_accuracy: 0.5910 - lr: 3.9063e-04\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6902 - accuracy: 0.9895 - val_loss: 3.2928 - val_accuracy: 0.5905 - lr: 3.9063e-04\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6891 - accuracy: 0.9906 - val_loss: 3.2794 - val_accuracy: 0.5945 - lr: 3.9063e-04\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6848 - accuracy: 0.9914 - val_loss: 3.2819 - val_accuracy: 0.5935 - lr: 3.9063e-04\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6843 - accuracy: 0.9911 - val_loss: 3.2687 - val_accuracy: 0.5960 - lr: 3.9063e-04\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6854 - accuracy: 0.9904 - val_loss: 3.2754 - val_accuracy: 0.5930 - lr: 3.9063e-04\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6838 - accuracy: 0.9915 - val_loss: 3.2899 - val_accuracy: 0.5935 - lr: 3.9063e-04\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6832 - accuracy: 0.9912 - val_loss: 3.2918 - val_accuracy: 0.5925 - lr: 3.9063e-04\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6837 - accuracy: 0.9909 - val_loss: 3.3007 - val_accuracy: 0.5910 - lr: 3.9063e-04\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6910 - accuracy: 0.9886 - val_loss: 3.2823 - val_accuracy: 0.5905 - lr: 3.9063e-04\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6909 - accuracy: 0.9877 - val_loss: 3.2886 - val_accuracy: 0.5905 - lr: 3.9063e-04\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6838 - accuracy: 0.9916 - val_loss: 3.2885 - val_accuracy: 0.5900 - lr: 3.9063e-04\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6827 - accuracy: 0.9909 - val_loss: 3.2960 - val_accuracy: 0.5905 - lr: 3.9063e-04\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6828 - accuracy: 0.9911 - val_loss: 3.2786 - val_accuracy: 0.5895 - lr: 3.9063e-04\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6818 - accuracy: 0.9905 - val_loss: 3.2785 - val_accuracy: 0.5895 - lr: 1.9531e-04\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6852 - accuracy: 0.9904 - val_loss: 3.2768 - val_accuracy: 0.5895 - lr: 1.9531e-04\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6807 - accuracy: 0.9914 - val_loss: 3.2718 - val_accuracy: 0.5905 - lr: 1.9531e-04\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6832 - accuracy: 0.9902 - val_loss: 3.2576 - val_accuracy: 0.5920 - lr: 1.9531e-04\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6836 - accuracy: 0.9908 - val_loss: 3.2534 - val_accuracy: 0.5960 - lr: 1.9531e-04\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6766 - accuracy: 0.9924 - val_loss: 3.2541 - val_accuracy: 0.5925 - lr: 1.9531e-04\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6802 - accuracy: 0.9910 - val_loss: 3.2633 - val_accuracy: 0.5945 - lr: 1.9531e-04\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6832 - accuracy: 0.9899 - val_loss: 3.2771 - val_accuracy: 0.5910 - lr: 1.9531e-04\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6811 - accuracy: 0.9912 - val_loss: 3.2773 - val_accuracy: 0.5940 - lr: 1.9531e-04\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6827 - accuracy: 0.9899 - val_loss: 3.2667 - val_accuracy: 0.5925 - lr: 1.9531e-04\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6857 - accuracy: 0.9901 - val_loss: 3.2701 - val_accuracy: 0.5940 - lr: 1.9531e-04\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6787 - accuracy: 0.9924 - val_loss: 3.2660 - val_accuracy: 0.5925 - lr: 1.9531e-04\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6798 - accuracy: 0.9923 - val_loss: 3.2708 - val_accuracy: 0.5925 - lr: 1.9531e-04\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6822 - accuracy: 0.9905 - val_loss: 3.2610 - val_accuracy: 0.5925 - lr: 1.9531e-04\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6765 - accuracy: 0.9926 - val_loss: 3.2631 - val_accuracy: 0.5945 - lr: 1.9531e-04\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6800 - accuracy: 0.9902 - val_loss: 3.2704 - val_accuracy: 0.5930 - lr: 1.9531e-04\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6844 - accuracy: 0.9896 - val_loss: 3.2629 - val_accuracy: 0.5960 - lr: 1.9531e-04\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6772 - accuracy: 0.9921 - val_loss: 3.2603 - val_accuracy: 0.5950 - lr: 1.9531e-04\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6756 - accuracy: 0.9926 - val_loss: 3.2599 - val_accuracy: 0.5965 - lr: 1.9531e-04\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6818 - accuracy: 0.9887 - val_loss: 3.2699 - val_accuracy: 0.5965 - lr: 1.9531e-04\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2699 - accuracy: 0.5965\n",
      "Score for fold 3: loss of 3.2698564529418945; accuracy of 59.64999794960022%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 32ms/step - loss: 25.2611 - accuracy: 0.0402 - val_loss: 86222.6406 - val_accuracy: 0.0180 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 27.8595 - accuracy: 0.0570 - val_loss: 539.0282 - val_accuracy: 0.0210 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 24.6014 - accuracy: 0.0804 - val_loss: 26.1044 - val_accuracy: 0.0755 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 22.0821 - accuracy: 0.0806 - val_loss: 21.4513 - val_accuracy: 0.0305 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 19.6561 - accuracy: 0.1041 - val_loss: 19.7978 - val_accuracy: 0.0240 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 17.5520 - accuracy: 0.1196 - val_loss: 17.8603 - val_accuracy: 0.0300 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 15.7538 - accuracy: 0.1370 - val_loss: 16.3503 - val_accuracy: 0.0325 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 14.1382 - accuracy: 0.1581 - val_loss: 15.1969 - val_accuracy: 0.0185 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 12.7471 - accuracy: 0.1785 - val_loss: 14.2137 - val_accuracy: 0.0380 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 11.5162 - accuracy: 0.1865 - val_loss: 13.0057 - val_accuracy: 0.0375 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 10.4444 - accuracy: 0.2074 - val_loss: 12.5749 - val_accuracy: 0.0295 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 9.5213 - accuracy: 0.2001 - val_loss: 11.3101 - val_accuracy: 0.0230 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 8.7031 - accuracy: 0.2140 - val_loss: 10.3201 - val_accuracy: 0.0560 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.9779 - accuracy: 0.2265 - val_loss: 9.0716 - val_accuracy: 0.0460 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.3109 - accuracy: 0.2369 - val_loss: 9.1817 - val_accuracy: 0.0270 - lr: 0.1000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.7607 - accuracy: 0.2469 - val_loss: 7.8701 - val_accuracy: 0.0670 - lr: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.2728 - accuracy: 0.2598 - val_loss: 7.4743 - val_accuracy: 0.0805 - lr: 0.1000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.8233 - accuracy: 0.2657 - val_loss: 8.2473 - val_accuracy: 0.0590 - lr: 0.1000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.4287 - accuracy: 0.2858 - val_loss: 6.9690 - val_accuracy: 0.0545 - lr: 0.1000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 5.0857 - accuracy: 0.2949 - val_loss: 6.2059 - val_accuracy: 0.0930 - lr: 0.1000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.7827 - accuracy: 0.3085 - val_loss: 6.0947 - val_accuracy: 0.0915 - lr: 0.0500\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.5727 - accuracy: 0.3304 - val_loss: 5.8639 - val_accuracy: 0.1215 - lr: 0.0500\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.4020 - accuracy: 0.3430 - val_loss: 5.6759 - val_accuracy: 0.1130 - lr: 0.0500\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.2441 - accuracy: 0.3517 - val_loss: 5.3978 - val_accuracy: 0.1350 - lr: 0.0500\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.1046 - accuracy: 0.3620 - val_loss: 4.9122 - val_accuracy: 0.1995 - lr: 0.0500\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.9697 - accuracy: 0.3750 - val_loss: 4.5911 - val_accuracy: 0.2115 - lr: 0.0500\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.8730 - accuracy: 0.3721 - val_loss: 4.8051 - val_accuracy: 0.2045 - lr: 0.0500\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.7362 - accuracy: 0.3880 - val_loss: 4.8166 - val_accuracy: 0.1755 - lr: 0.0500\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.6375 - accuracy: 0.3981 - val_loss: 4.7253 - val_accuracy: 0.1870 - lr: 0.0500\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.5184 - accuracy: 0.4129 - val_loss: 4.3784 - val_accuracy: 0.2235 - lr: 0.0500\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.4498 - accuracy: 0.4139 - val_loss: 4.9210 - val_accuracy: 0.1530 - lr: 0.0500\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.3296 - accuracy: 0.4271 - val_loss: 4.5942 - val_accuracy: 0.1985 - lr: 0.0500\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2412 - accuracy: 0.4279 - val_loss: 4.0154 - val_accuracy: 0.2710 - lr: 0.0500\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.1497 - accuracy: 0.4467 - val_loss: 4.1682 - val_accuracy: 0.2555 - lr: 0.0500\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.0482 - accuracy: 0.4593 - val_loss: 3.6723 - val_accuracy: 0.3040 - lr: 0.0500\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9900 - accuracy: 0.4615 - val_loss: 3.9308 - val_accuracy: 0.2695 - lr: 0.0500\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9105 - accuracy: 0.4709 - val_loss: 4.0374 - val_accuracy: 0.2740 - lr: 0.0500\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8647 - accuracy: 0.4852 - val_loss: 4.4784 - val_accuracy: 0.2205 - lr: 0.0500\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8014 - accuracy: 0.4882 - val_loss: 3.9641 - val_accuracy: 0.2655 - lr: 0.0500\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7109 - accuracy: 0.5070 - val_loss: 3.3135 - val_accuracy: 0.3630 - lr: 0.0500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.5892 - accuracy: 0.5309 - val_loss: 3.4833 - val_accuracy: 0.3415 - lr: 0.0250\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4798 - accuracy: 0.5569 - val_loss: 3.2381 - val_accuracy: 0.3770 - lr: 0.0250\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3996 - accuracy: 0.5641 - val_loss: 3.3779 - val_accuracy: 0.3385 - lr: 0.0250\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3532 - accuracy: 0.5778 - val_loss: 3.2650 - val_accuracy: 0.3725 - lr: 0.0250\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3136 - accuracy: 0.5828 - val_loss: 3.2780 - val_accuracy: 0.3830 - lr: 0.0250\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2803 - accuracy: 0.5922 - val_loss: 3.0362 - val_accuracy: 0.4025 - lr: 0.0250\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2396 - accuracy: 0.5961 - val_loss: 3.3045 - val_accuracy: 0.3500 - lr: 0.0250\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1795 - accuracy: 0.6044 - val_loss: 3.0767 - val_accuracy: 0.3940 - lr: 0.0250\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1395 - accuracy: 0.6176 - val_loss: 3.0389 - val_accuracy: 0.4100 - lr: 0.0250\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1149 - accuracy: 0.6221 - val_loss: 3.2892 - val_accuracy: 0.3880 - lr: 0.0250\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0998 - accuracy: 0.6200 - val_loss: 2.9616 - val_accuracy: 0.4325 - lr: 0.0250\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0592 - accuracy: 0.6336 - val_loss: 3.3355 - val_accuracy: 0.3690 - lr: 0.0250\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0585 - accuracy: 0.6374 - val_loss: 3.0355 - val_accuracy: 0.4295 - lr: 0.0250\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0432 - accuracy: 0.6300 - val_loss: 2.9185 - val_accuracy: 0.4405 - lr: 0.0250\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9783 - accuracy: 0.6482 - val_loss: 3.3664 - val_accuracy: 0.3675 - lr: 0.0250\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9720 - accuracy: 0.6507 - val_loss: 3.1160 - val_accuracy: 0.4165 - lr: 0.0250\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9270 - accuracy: 0.6631 - val_loss: 2.9903 - val_accuracy: 0.4270 - lr: 0.0250\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8950 - accuracy: 0.6718 - val_loss: 2.9600 - val_accuracy: 0.4590 - lr: 0.0250\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9026 - accuracy: 0.6665 - val_loss: 3.3634 - val_accuracy: 0.3990 - lr: 0.0250\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.8813 - accuracy: 0.6749 - val_loss: 3.0486 - val_accuracy: 0.4295 - lr: 0.0250\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7211 - accuracy: 0.7168 - val_loss: 2.9680 - val_accuracy: 0.4485 - lr: 0.0125\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.6065 - accuracy: 0.7565 - val_loss: 2.7991 - val_accuracy: 0.4860 - lr: 0.0125\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5852 - accuracy: 0.7539 - val_loss: 2.8530 - val_accuracy: 0.4835 - lr: 0.0125\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5291 - accuracy: 0.7741 - val_loss: 3.0916 - val_accuracy: 0.4285 - lr: 0.0125\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5169 - accuracy: 0.7749 - val_loss: 3.0220 - val_accuracy: 0.4710 - lr: 0.0125\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4911 - accuracy: 0.7820 - val_loss: 3.1196 - val_accuracy: 0.4685 - lr: 0.0125\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4910 - accuracy: 0.7799 - val_loss: 2.8101 - val_accuracy: 0.5170 - lr: 0.0125\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4583 - accuracy: 0.7949 - val_loss: 3.0218 - val_accuracy: 0.4800 - lr: 0.0125\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4205 - accuracy: 0.8019 - val_loss: 2.9189 - val_accuracy: 0.4900 - lr: 0.0125\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4163 - accuracy: 0.8000 - val_loss: 2.9219 - val_accuracy: 0.5015 - lr: 0.0125\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3935 - accuracy: 0.8058 - val_loss: 3.1685 - val_accuracy: 0.4640 - lr: 0.0125\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3889 - accuracy: 0.8105 - val_loss: 3.0518 - val_accuracy: 0.4790 - lr: 0.0125\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3548 - accuracy: 0.8141 - val_loss: 3.1130 - val_accuracy: 0.4795 - lr: 0.0125\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3616 - accuracy: 0.8099 - val_loss: 3.1052 - val_accuracy: 0.4850 - lr: 0.0125\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3716 - accuracy: 0.8114 - val_loss: 3.1365 - val_accuracy: 0.4870 - lr: 0.0125\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3286 - accuracy: 0.8244 - val_loss: 3.1691 - val_accuracy: 0.4760 - lr: 0.0125\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3389 - accuracy: 0.8205 - val_loss: 3.0356 - val_accuracy: 0.5085 - lr: 0.0125\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2948 - accuracy: 0.8309 - val_loss: 3.0803 - val_accuracy: 0.4985 - lr: 0.0125\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3010 - accuracy: 0.8332 - val_loss: 2.9695 - val_accuracy: 0.5280 - lr: 0.0125\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3014 - accuracy: 0.8299 - val_loss: 3.2762 - val_accuracy: 0.4735 - lr: 0.0125\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2157 - accuracy: 0.8616 - val_loss: 2.9029 - val_accuracy: 0.5300 - lr: 0.0063\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1296 - accuracy: 0.8907 - val_loss: 2.8770 - val_accuracy: 0.5380 - lr: 0.0063\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0797 - accuracy: 0.9018 - val_loss: 2.8802 - val_accuracy: 0.5400 - lr: 0.0063\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0572 - accuracy: 0.9106 - val_loss: 3.0692 - val_accuracy: 0.5145 - lr: 0.0063\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0647 - accuracy: 0.9009 - val_loss: 2.9418 - val_accuracy: 0.5315 - lr: 0.0063\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0264 - accuracy: 0.9161 - val_loss: 2.9926 - val_accuracy: 0.5375 - lr: 0.0063\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0227 - accuracy: 0.9121 - val_loss: 3.1279 - val_accuracy: 0.5310 - lr: 0.0063\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0245 - accuracy: 0.9143 - val_loss: 3.0871 - val_accuracy: 0.5285 - lr: 0.0063\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9839 - accuracy: 0.9244 - val_loss: 3.0422 - val_accuracy: 0.5335 - lr: 0.0063\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0070 - accuracy: 0.9125 - val_loss: 3.1831 - val_accuracy: 0.5285 - lr: 0.0063\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9841 - accuracy: 0.9212 - val_loss: 3.1705 - val_accuracy: 0.5315 - lr: 0.0063\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9633 - accuracy: 0.9275 - val_loss: 3.1948 - val_accuracy: 0.5225 - lr: 0.0063\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9711 - accuracy: 0.9265 - val_loss: 3.2424 - val_accuracy: 0.5115 - lr: 0.0063\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9673 - accuracy: 0.9266 - val_loss: 3.4194 - val_accuracy: 0.5040 - lr: 0.0063\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9464 - accuracy: 0.9323 - val_loss: 3.1515 - val_accuracy: 0.5420 - lr: 0.0063\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9457 - accuracy: 0.9258 - val_loss: 3.3799 - val_accuracy: 0.5190 - lr: 0.0063\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.9380 - accuracy: 0.9310 - val_loss: 3.2397 - val_accuracy: 0.5460 - lr: 0.0063\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9377 - accuracy: 0.9320 - val_loss: 3.2776 - val_accuracy: 0.5335 - lr: 0.0063\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9370 - accuracy: 0.9315 - val_loss: 3.2721 - val_accuracy: 0.5340 - lr: 0.0063\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9237 - accuracy: 0.9359 - val_loss: 3.2436 - val_accuracy: 0.5375 - lr: 0.0063\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8877 - accuracy: 0.9408 - val_loss: 3.3140 - val_accuracy: 0.5350 - lr: 0.0031\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8446 - accuracy: 0.9570 - val_loss: 3.1948 - val_accuracy: 0.5455 - lr: 0.0031\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8365 - accuracy: 0.9613 - val_loss: 3.1502 - val_accuracy: 0.5505 - lr: 0.0031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8319 - accuracy: 0.9614 - val_loss: 3.1542 - val_accuracy: 0.5520 - lr: 0.0031\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8199 - accuracy: 0.9656 - val_loss: 3.0761 - val_accuracy: 0.5670 - lr: 0.0031\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8136 - accuracy: 0.9660 - val_loss: 3.2160 - val_accuracy: 0.5545 - lr: 0.0031\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8021 - accuracy: 0.9689 - val_loss: 3.3017 - val_accuracy: 0.5385 - lr: 0.0031\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8063 - accuracy: 0.9661 - val_loss: 3.1549 - val_accuracy: 0.5650 - lr: 0.0031\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7910 - accuracy: 0.9724 - val_loss: 3.2268 - val_accuracy: 0.5580 - lr: 0.0031\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7884 - accuracy: 0.9691 - val_loss: 3.2135 - val_accuracy: 0.5595 - lr: 0.0031\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7892 - accuracy: 0.9688 - val_loss: 3.3572 - val_accuracy: 0.5445 - lr: 0.0031\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7767 - accuracy: 0.9732 - val_loss: 3.3353 - val_accuracy: 0.5465 - lr: 0.0031\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7757 - accuracy: 0.9739 - val_loss: 3.2044 - val_accuracy: 0.5545 - lr: 0.0031\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7912 - accuracy: 0.9659 - val_loss: 3.2423 - val_accuracy: 0.5540 - lr: 0.0031\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7656 - accuracy: 0.9753 - val_loss: 3.2574 - val_accuracy: 0.5530 - lr: 0.0031\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7632 - accuracy: 0.9749 - val_loss: 3.3419 - val_accuracy: 0.5525 - lr: 0.0031\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7569 - accuracy: 0.9772 - val_loss: 3.3127 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7660 - accuracy: 0.9720 - val_loss: 3.4375 - val_accuracy: 0.5420 - lr: 0.0031\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7695 - accuracy: 0.9700 - val_loss: 3.2521 - val_accuracy: 0.5655 - lr: 0.0031\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7624 - accuracy: 0.9709 - val_loss: 3.3667 - val_accuracy: 0.5425 - lr: 0.0031\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7453 - accuracy: 0.9780 - val_loss: 3.3359 - val_accuracy: 0.5525 - lr: 0.0016\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7275 - accuracy: 0.9824 - val_loss: 3.2994 - val_accuracy: 0.5640 - lr: 0.0016\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7350 - accuracy: 0.9800 - val_loss: 3.2719 - val_accuracy: 0.5640 - lr: 0.0016\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7244 - accuracy: 0.9833 - val_loss: 3.2863 - val_accuracy: 0.5615 - lr: 0.0016\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7245 - accuracy: 0.9812 - val_loss: 3.2843 - val_accuracy: 0.5630 - lr: 0.0016\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7289 - accuracy: 0.9780 - val_loss: 3.2990 - val_accuracy: 0.5580 - lr: 0.0016\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7252 - accuracy: 0.9815 - val_loss: 3.3130 - val_accuracy: 0.5620 - lr: 0.0016\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7189 - accuracy: 0.9820 - val_loss: 3.3131 - val_accuracy: 0.5655 - lr: 0.0016\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7270 - accuracy: 0.9795 - val_loss: 3.3351 - val_accuracy: 0.5570 - lr: 0.0016\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7194 - accuracy: 0.9825 - val_loss: 3.3189 - val_accuracy: 0.5600 - lr: 0.0016\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7175 - accuracy: 0.9818 - val_loss: 3.2920 - val_accuracy: 0.5635 - lr: 0.0016\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7170 - accuracy: 0.9799 - val_loss: 3.3081 - val_accuracy: 0.5590 - lr: 0.0016\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7158 - accuracy: 0.9815 - val_loss: 3.3077 - val_accuracy: 0.5600 - lr: 0.0016\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7013 - accuracy: 0.9860 - val_loss: 3.3456 - val_accuracy: 0.5510 - lr: 0.0016\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7023 - accuracy: 0.9861 - val_loss: 3.3557 - val_accuracy: 0.5555 - lr: 0.0016\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6951 - accuracy: 0.9876 - val_loss: 3.3148 - val_accuracy: 0.5645 - lr: 0.0016\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7025 - accuracy: 0.9839 - val_loss: 3.3279 - val_accuracy: 0.5565 - lr: 0.0016\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6965 - accuracy: 0.9845 - val_loss: 3.3316 - val_accuracy: 0.5600 - lr: 0.0016\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6928 - accuracy: 0.9871 - val_loss: 3.3297 - val_accuracy: 0.5630 - lr: 0.0016\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6964 - accuracy: 0.9868 - val_loss: 3.3466 - val_accuracy: 0.5630 - lr: 0.0016\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6937 - accuracy: 0.9847 - val_loss: 3.3796 - val_accuracy: 0.5625 - lr: 7.8125e-04\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6942 - accuracy: 0.9849 - val_loss: 3.3314 - val_accuracy: 0.5645 - lr: 7.8125e-04\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6939 - accuracy: 0.9843 - val_loss: 3.2787 - val_accuracy: 0.5710 - lr: 7.8125e-04\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6967 - accuracy: 0.9840 - val_loss: 3.3172 - val_accuracy: 0.5695 - lr: 7.8125e-04\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6874 - accuracy: 0.9861 - val_loss: 3.2967 - val_accuracy: 0.5560 - lr: 7.8125e-04\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6839 - accuracy: 0.9898 - val_loss: 3.3349 - val_accuracy: 0.5650 - lr: 7.8125e-04\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6898 - accuracy: 0.9865 - val_loss: 3.2972 - val_accuracy: 0.5675 - lr: 7.8125e-04\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6854 - accuracy: 0.9869 - val_loss: 3.3395 - val_accuracy: 0.5660 - lr: 7.8125e-04\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6791 - accuracy: 0.9887 - val_loss: 3.3264 - val_accuracy: 0.5655 - lr: 7.8125e-04\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6839 - accuracy: 0.9866 - val_loss: 3.3329 - val_accuracy: 0.5635 - lr: 7.8125e-04\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6763 - accuracy: 0.9889 - val_loss: 3.3257 - val_accuracy: 0.5620 - lr: 7.8125e-04\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6783 - accuracy: 0.9883 - val_loss: 3.3138 - val_accuracy: 0.5625 - lr: 7.8125e-04\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6790 - accuracy: 0.9891 - val_loss: 3.3869 - val_accuracy: 0.5595 - lr: 7.8125e-04\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6770 - accuracy: 0.9872 - val_loss: 3.3394 - val_accuracy: 0.5630 - lr: 7.8125e-04\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6781 - accuracy: 0.9885 - val_loss: 3.3063 - val_accuracy: 0.5665 - lr: 7.8125e-04\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6734 - accuracy: 0.9898 - val_loss: 3.3189 - val_accuracy: 0.5630 - lr: 7.8125e-04\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6783 - accuracy: 0.9889 - val_loss: 3.3340 - val_accuracy: 0.5645 - lr: 7.8125e-04\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6674 - accuracy: 0.9901 - val_loss: 3.3453 - val_accuracy: 0.5640 - lr: 7.8125e-04\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6737 - accuracy: 0.9870 - val_loss: 3.3078 - val_accuracy: 0.5640 - lr: 7.8125e-04\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6736 - accuracy: 0.9890 - val_loss: 3.3333 - val_accuracy: 0.5650 - lr: 7.8125e-04\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6724 - accuracy: 0.9893 - val_loss: 3.3298 - val_accuracy: 0.5625 - lr: 3.9063e-04\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6735 - accuracy: 0.9885 - val_loss: 3.3372 - val_accuracy: 0.5605 - lr: 3.9063e-04\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6684 - accuracy: 0.9894 - val_loss: 3.3436 - val_accuracy: 0.5610 - lr: 3.9063e-04\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6722 - accuracy: 0.9889 - val_loss: 3.3150 - val_accuracy: 0.5630 - lr: 3.9063e-04\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6685 - accuracy: 0.9894 - val_loss: 3.3338 - val_accuracy: 0.5595 - lr: 3.9063e-04\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6677 - accuracy: 0.9908 - val_loss: 3.3342 - val_accuracy: 0.5610 - lr: 3.9063e-04\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6665 - accuracy: 0.9900 - val_loss: 3.3311 - val_accuracy: 0.5600 - lr: 3.9063e-04\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6700 - accuracy: 0.9894 - val_loss: 3.3289 - val_accuracy: 0.5560 - lr: 3.9063e-04\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6654 - accuracy: 0.9902 - val_loss: 3.3229 - val_accuracy: 0.5600 - lr: 3.9063e-04\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6694 - accuracy: 0.9874 - val_loss: 3.3156 - val_accuracy: 0.5625 - lr: 3.9063e-04\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6634 - accuracy: 0.9915 - val_loss: 3.3406 - val_accuracy: 0.5610 - lr: 3.9063e-04\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6670 - accuracy: 0.9895 - val_loss: 3.3237 - val_accuracy: 0.5630 - lr: 3.9063e-04\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6610 - accuracy: 0.9918 - val_loss: 3.3284 - val_accuracy: 0.5610 - lr: 3.9063e-04\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6610 - accuracy: 0.9909 - val_loss: 3.3614 - val_accuracy: 0.5600 - lr: 3.9063e-04\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6641 - accuracy: 0.9910 - val_loss: 3.3597 - val_accuracy: 0.5585 - lr: 3.9063e-04\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6664 - accuracy: 0.9894 - val_loss: 3.3333 - val_accuracy: 0.5620 - lr: 3.9063e-04\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6615 - accuracy: 0.9916 - val_loss: 3.3361 - val_accuracy: 0.5625 - lr: 3.9063e-04\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6641 - accuracy: 0.9902 - val_loss: 3.3418 - val_accuracy: 0.5600 - lr: 3.9063e-04\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6610 - accuracy: 0.9908 - val_loss: 3.3378 - val_accuracy: 0.5630 - lr: 3.9063e-04\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6633 - accuracy: 0.9900 - val_loss: 3.3290 - val_accuracy: 0.5620 - lr: 3.9063e-04\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6587 - accuracy: 0.9912 - val_loss: 3.3246 - val_accuracy: 0.5620 - lr: 1.9531e-04\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6634 - accuracy: 0.9879 - val_loss: 3.3167 - val_accuracy: 0.5635 - lr: 1.9531e-04\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6643 - accuracy: 0.9909 - val_loss: 3.3192 - val_accuracy: 0.5595 - lr: 1.9531e-04\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6646 - accuracy: 0.9896 - val_loss: 3.3203 - val_accuracy: 0.5585 - lr: 1.9531e-04\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6621 - accuracy: 0.9899 - val_loss: 3.3162 - val_accuracy: 0.5595 - lr: 1.9531e-04\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6583 - accuracy: 0.9914 - val_loss: 3.3280 - val_accuracy: 0.5640 - lr: 1.9531e-04\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6547 - accuracy: 0.9925 - val_loss: 3.3301 - val_accuracy: 0.5605 - lr: 1.9531e-04\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6609 - accuracy: 0.9904 - val_loss: 3.3185 - val_accuracy: 0.5635 - lr: 1.9531e-04\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6600 - accuracy: 0.9896 - val_loss: 3.3158 - val_accuracy: 0.5620 - lr: 1.9531e-04\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6598 - accuracy: 0.9908 - val_loss: 3.3132 - val_accuracy: 0.5630 - lr: 1.9531e-04\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6618 - accuracy: 0.9900 - val_loss: 3.3289 - val_accuracy: 0.5610 - lr: 1.9531e-04\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6529 - accuracy: 0.9934 - val_loss: 3.3285 - val_accuracy: 0.5640 - lr: 1.9531e-04\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6565 - accuracy: 0.9916 - val_loss: 3.3258 - val_accuracy: 0.5625 - lr: 1.9531e-04\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6557 - accuracy: 0.9904 - val_loss: 3.3365 - val_accuracy: 0.5630 - lr: 1.9531e-04\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6572 - accuracy: 0.9912 - val_loss: 3.3245 - val_accuracy: 0.5650 - lr: 1.9531e-04\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6589 - accuracy: 0.9910 - val_loss: 3.3253 - val_accuracy: 0.5625 - lr: 1.9531e-04\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6554 - accuracy: 0.9909 - val_loss: 3.3336 - val_accuracy: 0.5595 - lr: 1.9531e-04\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6578 - accuracy: 0.9906 - val_loss: 3.3264 - val_accuracy: 0.5615 - lr: 1.9531e-04\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6559 - accuracy: 0.9918 - val_loss: 3.3439 - val_accuracy: 0.5580 - lr: 1.9531e-04\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6565 - accuracy: 0.9910 - val_loss: 3.3308 - val_accuracy: 0.5610 - lr: 1.9531e-04\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.3308 - accuracy: 0.5610\n",
      "Score for fold 4: loss of 3.330843925476074; accuracy of 56.09999895095825%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/200\n",
      "63/63 [==============================] - 4s 37ms/step - loss: 27.2784 - accuracy: 0.0440 - val_loss: 13893.0381 - val_accuracy: 0.0205 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 29.8983 - accuracy: 0.0735 - val_loss: 67.5390 - val_accuracy: 0.0320 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 26.4723 - accuracy: 0.0882 - val_loss: 25.1918 - val_accuracy: 0.0750 - lr: 0.1000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 23.5672 - accuracy: 0.1189 - val_loss: 22.9106 - val_accuracy: 0.0590 - lr: 0.1000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 21.0009 - accuracy: 0.1391 - val_loss: 20.6782 - val_accuracy: 0.0550 - lr: 0.1000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 18.8213 - accuracy: 0.1573 - val_loss: 19.5470 - val_accuracy: 0.0285 - lr: 0.1000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 16.8525 - accuracy: 0.1771 - val_loss: 17.9953 - val_accuracy: 0.0200 - lr: 0.1000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 15.1542 - accuracy: 0.1857 - val_loss: 16.4595 - val_accuracy: 0.0315 - lr: 0.1000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 13.6934 - accuracy: 0.1876 - val_loss: 15.2367 - val_accuracy: 0.0205 - lr: 0.1000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 12.3435 - accuracy: 0.2026 - val_loss: 13.6883 - val_accuracy: 0.0360 - lr: 0.1000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 11.1638 - accuracy: 0.2129 - val_loss: 12.2839 - val_accuracy: 0.0445 - lr: 0.1000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 10.1178 - accuracy: 0.2215 - val_loss: 11.3853 - val_accuracy: 0.0590 - lr: 0.1000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 9.2371 - accuracy: 0.2414 - val_loss: 10.6307 - val_accuracy: 0.0485 - lr: 0.1000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 8.4174 - accuracy: 0.2506 - val_loss: 9.6964 - val_accuracy: 0.0620 - lr: 0.1000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.7431 - accuracy: 0.2541 - val_loss: 9.1596 - val_accuracy: 0.0810 - lr: 0.1000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 7.1069 - accuracy: 0.2695 - val_loss: 8.5328 - val_accuracy: 0.0635 - lr: 0.1000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 6.5739 - accuracy: 0.2819 - val_loss: 7.3572 - val_accuracy: 0.1260 - lr: 0.1000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 6.1211 - accuracy: 0.2831 - val_loss: 7.2540 - val_accuracy: 0.0825 - lr: 0.1000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 5.7081 - accuracy: 0.2962 - val_loss: 7.0162 - val_accuracy: 0.0930 - lr: 0.1000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 5.3336 - accuracy: 0.3025 - val_loss: 6.5578 - val_accuracy: 0.1210 - lr: 0.1000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 4.9979 - accuracy: 0.3153 - val_loss: 6.4635 - val_accuracy: 0.1345 - lr: 0.0500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.7753 - accuracy: 0.3406 - val_loss: 6.1282 - val_accuracy: 0.1405 - lr: 0.0500\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.5905 - accuracy: 0.3531 - val_loss: 5.3370 - val_accuracy: 0.1875 - lr: 0.0500\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 4.4269 - accuracy: 0.3645 - val_loss: 5.1977 - val_accuracy: 0.2200 - lr: 0.0500\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 4.2749 - accuracy: 0.3715 - val_loss: 6.1902 - val_accuracy: 0.1240 - lr: 0.0500\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 4.1509 - accuracy: 0.3701 - val_loss: 4.9220 - val_accuracy: 0.2240 - lr: 0.0500\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 4.0282 - accuracy: 0.3798 - val_loss: 5.1439 - val_accuracy: 0.1955 - lr: 0.0500\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 3.9146 - accuracy: 0.3844 - val_loss: 5.2036 - val_accuracy: 0.1620 - lr: 0.0500\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.7708 - accuracy: 0.4005 - val_loss: 5.5726 - val_accuracy: 0.1640 - lr: 0.0500\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.6746 - accuracy: 0.4056 - val_loss: 4.3937 - val_accuracy: 0.2620 - lr: 0.0500\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.5556 - accuracy: 0.4136 - val_loss: 4.6563 - val_accuracy: 0.2390 - lr: 0.0500\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.4579 - accuracy: 0.4241 - val_loss: 4.0057 - val_accuracy: 0.2995 - lr: 0.0500\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.3587 - accuracy: 0.4342 - val_loss: 4.2470 - val_accuracy: 0.2985 - lr: 0.0500\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.2775 - accuracy: 0.4423 - val_loss: 4.8951 - val_accuracy: 0.1945 - lr: 0.0500\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.1589 - accuracy: 0.4647 - val_loss: 4.0558 - val_accuracy: 0.2950 - lr: 0.0500\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.0926 - accuracy: 0.4681 - val_loss: 4.0905 - val_accuracy: 0.2630 - lr: 0.0500\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 3.0265 - accuracy: 0.4676 - val_loss: 3.4343 - val_accuracy: 0.3950 - lr: 0.0500\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.9261 - accuracy: 0.4850 - val_loss: 3.3716 - val_accuracy: 0.3930 - lr: 0.0500\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.8489 - accuracy: 0.4956 - val_loss: 4.0047 - val_accuracy: 0.2985 - lr: 0.0500\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.7964 - accuracy: 0.5054 - val_loss: 3.9119 - val_accuracy: 0.3055 - lr: 0.0500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.6192 - accuracy: 0.5403 - val_loss: 3.2770 - val_accuracy: 0.4215 - lr: 0.0250\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4963 - accuracy: 0.5735 - val_loss: 3.2115 - val_accuracy: 0.4055 - lr: 0.0250\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.4292 - accuracy: 0.5855 - val_loss: 3.3353 - val_accuracy: 0.4120 - lr: 0.0250\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3763 - accuracy: 0.5968 - val_loss: 3.2660 - val_accuracy: 0.4025 - lr: 0.0250\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.3767 - accuracy: 0.5928 - val_loss: 3.2118 - val_accuracy: 0.4120 - lr: 0.0250\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2869 - accuracy: 0.6106 - val_loss: 3.2213 - val_accuracy: 0.4390 - lr: 0.0250\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2576 - accuracy: 0.6121 - val_loss: 3.1495 - val_accuracy: 0.4345 - lr: 0.0250\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.2363 - accuracy: 0.6194 - val_loss: 3.5231 - val_accuracy: 0.3810 - lr: 0.0250\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1856 - accuracy: 0.6258 - val_loss: 2.9896 - val_accuracy: 0.4495 - lr: 0.0250\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1643 - accuracy: 0.6280 - val_loss: 3.1445 - val_accuracy: 0.4385 - lr: 0.0250\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.1279 - accuracy: 0.6385 - val_loss: 3.0618 - val_accuracy: 0.4565 - lr: 0.0250\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0931 - accuracy: 0.6455 - val_loss: 3.2502 - val_accuracy: 0.4045 - lr: 0.0250\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0585 - accuracy: 0.6503 - val_loss: 3.0318 - val_accuracy: 0.4770 - lr: 0.0250\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0608 - accuracy: 0.6479 - val_loss: 3.2829 - val_accuracy: 0.3965 - lr: 0.0250\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 2.0333 - accuracy: 0.6562 - val_loss: 3.6780 - val_accuracy: 0.3735 - lr: 0.0250\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9626 - accuracy: 0.6721 - val_loss: 3.0533 - val_accuracy: 0.4665 - lr: 0.0250\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9467 - accuracy: 0.6812 - val_loss: 3.1474 - val_accuracy: 0.4495 - lr: 0.0250\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9841 - accuracy: 0.6630 - val_loss: 3.0430 - val_accuracy: 0.4570 - lr: 0.0250\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9139 - accuracy: 0.6835 - val_loss: 3.1267 - val_accuracy: 0.4690 - lr: 0.0250\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.9165 - accuracy: 0.6794 - val_loss: 3.0438 - val_accuracy: 0.4835 - lr: 0.0250\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.7400 - accuracy: 0.7365 - val_loss: 2.8127 - val_accuracy: 0.5240 - lr: 0.0125\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.6183 - accuracy: 0.7722 - val_loss: 3.0570 - val_accuracy: 0.4760 - lr: 0.0125\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.6023 - accuracy: 0.7691 - val_loss: 2.9685 - val_accuracy: 0.4855 - lr: 0.0125\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.5790 - accuracy: 0.7724 - val_loss: 2.7172 - val_accuracy: 0.5485 - lr: 0.0125\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.5347 - accuracy: 0.7829 - val_loss: 3.0287 - val_accuracy: 0.5065 - lr: 0.0125\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.5371 - accuracy: 0.7856 - val_loss: 2.8739 - val_accuracy: 0.5190 - lr: 0.0125\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.4864 - accuracy: 0.7947 - val_loss: 2.8020 - val_accuracy: 0.5380 - lr: 0.0125\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4875 - accuracy: 0.8027 - val_loss: 2.9889 - val_accuracy: 0.5280 - lr: 0.0125\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.4596 - accuracy: 0.8046 - val_loss: 2.9322 - val_accuracy: 0.5135 - lr: 0.0125\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.4706 - accuracy: 0.8023 - val_loss: 3.2776 - val_accuracy: 0.4830 - lr: 0.0125\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4367 - accuracy: 0.8144 - val_loss: 3.1502 - val_accuracy: 0.5070 - lr: 0.0125\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.4564 - accuracy: 0.8006 - val_loss: 3.2764 - val_accuracy: 0.4805 - lr: 0.0125\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4330 - accuracy: 0.8140 - val_loss: 3.0993 - val_accuracy: 0.5020 - lr: 0.0125\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3975 - accuracy: 0.8214 - val_loss: 3.1162 - val_accuracy: 0.4890 - lr: 0.0125\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.4020 - accuracy: 0.8196 - val_loss: 3.1388 - val_accuracy: 0.5095 - lr: 0.0125\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3846 - accuracy: 0.8175 - val_loss: 3.5033 - val_accuracy: 0.4775 - lr: 0.0125\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3935 - accuracy: 0.8205 - val_loss: 3.1515 - val_accuracy: 0.5150 - lr: 0.0125\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3542 - accuracy: 0.8356 - val_loss: 3.1101 - val_accuracy: 0.4995 - lr: 0.0125\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.3341 - accuracy: 0.8370 - val_loss: 3.1132 - val_accuracy: 0.5135 - lr: 0.0125\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.3333 - accuracy: 0.8376 - val_loss: 3.2183 - val_accuracy: 0.5020 - lr: 0.0125\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.2832 - accuracy: 0.8518 - val_loss: 2.9596 - val_accuracy: 0.5440 - lr: 0.0063\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1668 - accuracy: 0.8910 - val_loss: 2.9281 - val_accuracy: 0.5480 - lr: 0.0063\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1366 - accuracy: 0.8994 - val_loss: 2.9756 - val_accuracy: 0.5495 - lr: 0.0063\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.1150 - accuracy: 0.9021 - val_loss: 3.0702 - val_accuracy: 0.5360 - lr: 0.0063\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0848 - accuracy: 0.9124 - val_loss: 2.9985 - val_accuracy: 0.5500 - lr: 0.0063\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0785 - accuracy: 0.9110 - val_loss: 3.2121 - val_accuracy: 0.5185 - lr: 0.0063\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0797 - accuracy: 0.9094 - val_loss: 3.1783 - val_accuracy: 0.5400 - lr: 0.0063\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0589 - accuracy: 0.9191 - val_loss: 3.1596 - val_accuracy: 0.5315 - lr: 0.0063\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0582 - accuracy: 0.9124 - val_loss: 3.0223 - val_accuracy: 0.5680 - lr: 0.0063\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0513 - accuracy: 0.9166 - val_loss: 3.1894 - val_accuracy: 0.5465 - lr: 0.0063\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0435 - accuracy: 0.9202 - val_loss: 3.0684 - val_accuracy: 0.5490 - lr: 0.0063\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0382 - accuracy: 0.9174 - val_loss: 3.1809 - val_accuracy: 0.5515 - lr: 0.0063\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0496 - accuracy: 0.9106 - val_loss: 3.0418 - val_accuracy: 0.5670 - lr: 0.0063\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0123 - accuracy: 0.9258 - val_loss: 3.2179 - val_accuracy: 0.5475 - lr: 0.0063\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 1.0138 - accuracy: 0.9205 - val_loss: 3.1717 - val_accuracy: 0.5435 - lr: 0.0063\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9969 - accuracy: 0.9279 - val_loss: 3.1768 - val_accuracy: 0.5440 - lr: 0.0063\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0005 - accuracy: 0.9256 - val_loss: 3.2343 - val_accuracy: 0.5445 - lr: 0.0063\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 1.0055 - accuracy: 0.9224 - val_loss: 3.2790 - val_accuracy: 0.5440 - lr: 0.0063\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9893 - accuracy: 0.9298 - val_loss: 3.2462 - val_accuracy: 0.5405 - lr: 0.0063\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9754 - accuracy: 0.9304 - val_loss: 3.2687 - val_accuracy: 0.5530 - lr: 0.0063\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.9510 - accuracy: 0.9377 - val_loss: 3.1674 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.9182 - accuracy: 0.9477 - val_loss: 3.1231 - val_accuracy: 0.5705 - lr: 0.0031\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8919 - accuracy: 0.9550 - val_loss: 3.1967 - val_accuracy: 0.5605 - lr: 0.0031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8744 - accuracy: 0.9616 - val_loss: 3.1044 - val_accuracy: 0.5730 - lr: 0.0031\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8679 - accuracy: 0.9625 - val_loss: 3.2139 - val_accuracy: 0.5495 - lr: 0.0031\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8719 - accuracy: 0.9610 - val_loss: 3.1369 - val_accuracy: 0.5700 - lr: 0.0031\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8613 - accuracy: 0.9636 - val_loss: 3.2110 - val_accuracy: 0.5545 - lr: 0.0031\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8473 - accuracy: 0.9684 - val_loss: 3.2157 - val_accuracy: 0.5595 - lr: 0.0031\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8537 - accuracy: 0.9643 - val_loss: 3.1936 - val_accuracy: 0.5605 - lr: 0.0031\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8507 - accuracy: 0.9650 - val_loss: 3.2709 - val_accuracy: 0.5570 - lr: 0.0031\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8378 - accuracy: 0.9675 - val_loss: 3.2384 - val_accuracy: 0.5555 - lr: 0.0031\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8272 - accuracy: 0.9690 - val_loss: 3.2472 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8218 - accuracy: 0.9709 - val_loss: 3.3083 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8300 - accuracy: 0.9685 - val_loss: 3.4267 - val_accuracy: 0.5430 - lr: 0.0031\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8334 - accuracy: 0.9656 - val_loss: 3.3080 - val_accuracy: 0.5510 - lr: 0.0031\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8181 - accuracy: 0.9682 - val_loss: 3.1879 - val_accuracy: 0.5710 - lr: 0.0031\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8260 - accuracy: 0.9644 - val_loss: 3.3354 - val_accuracy: 0.5565 - lr: 0.0031\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.8093 - accuracy: 0.9712 - val_loss: 3.2191 - val_accuracy: 0.5600 - lr: 0.0031\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8094 - accuracy: 0.9700 - val_loss: 3.3737 - val_accuracy: 0.5470 - lr: 0.0031\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8030 - accuracy: 0.9691 - val_loss: 3.2835 - val_accuracy: 0.5595 - lr: 0.0031\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.8007 - accuracy: 0.9701 - val_loss: 3.2506 - val_accuracy: 0.5665 - lr: 0.0016\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7930 - accuracy: 0.9728 - val_loss: 3.1997 - val_accuracy: 0.5675 - lr: 0.0016\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7829 - accuracy: 0.9749 - val_loss: 3.2124 - val_accuracy: 0.5720 - lr: 0.0016\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7733 - accuracy: 0.9806 - val_loss: 3.2606 - val_accuracy: 0.5685 - lr: 0.0016\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7830 - accuracy: 0.9732 - val_loss: 3.1858 - val_accuracy: 0.5745 - lr: 0.0016\n",
      "Epoch 126/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7670 - accuracy: 0.9794 - val_loss: 3.2506 - val_accuracy: 0.5670 - lr: 0.0016\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7698 - accuracy: 0.9787 - val_loss: 3.2429 - val_accuracy: 0.5680 - lr: 0.0016\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7662 - accuracy: 0.9799 - val_loss: 3.2400 - val_accuracy: 0.5710 - lr: 0.0016\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7708 - accuracy: 0.9775 - val_loss: 3.2398 - val_accuracy: 0.5715 - lr: 0.0016\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7585 - accuracy: 0.9821 - val_loss: 3.2490 - val_accuracy: 0.5645 - lr: 0.0016\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7618 - accuracy: 0.9794 - val_loss: 3.2098 - val_accuracy: 0.5725 - lr: 0.0016\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7572 - accuracy: 0.9820 - val_loss: 3.2650 - val_accuracy: 0.5680 - lr: 0.0016\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7634 - accuracy: 0.9801 - val_loss: 3.2064 - val_accuracy: 0.5750 - lr: 0.0016\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7574 - accuracy: 0.9812 - val_loss: 3.3167 - val_accuracy: 0.5680 - lr: 0.0016\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7524 - accuracy: 0.9816 - val_loss: 3.2756 - val_accuracy: 0.5720 - lr: 0.0016\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7547 - accuracy: 0.9808 - val_loss: 3.2728 - val_accuracy: 0.5745 - lr: 0.0016\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7465 - accuracy: 0.9816 - val_loss: 3.2842 - val_accuracy: 0.5710 - lr: 0.0016\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7498 - accuracy: 0.9812 - val_loss: 3.3586 - val_accuracy: 0.5670 - lr: 0.0016\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7447 - accuracy: 0.9834 - val_loss: 3.3216 - val_accuracy: 0.5655 - lr: 0.0016\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7434 - accuracy: 0.9834 - val_loss: 3.3291 - val_accuracy: 0.5700 - lr: 0.0016\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7376 - accuracy: 0.9845 - val_loss: 3.2683 - val_accuracy: 0.5745 - lr: 7.8125e-04\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7375 - accuracy: 0.9856 - val_loss: 3.2953 - val_accuracy: 0.5760 - lr: 7.8125e-04\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7392 - accuracy: 0.9845 - val_loss: 3.2222 - val_accuracy: 0.5830 - lr: 7.8125e-04\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7402 - accuracy: 0.9816 - val_loss: 3.2681 - val_accuracy: 0.5795 - lr: 7.8125e-04\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7376 - accuracy: 0.9831 - val_loss: 3.2586 - val_accuracy: 0.5800 - lr: 7.8125e-04\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7321 - accuracy: 0.9861 - val_loss: 3.2318 - val_accuracy: 0.5820 - lr: 7.8125e-04\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7255 - accuracy: 0.9884 - val_loss: 3.2760 - val_accuracy: 0.5740 - lr: 7.8125e-04\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7253 - accuracy: 0.9869 - val_loss: 3.2859 - val_accuracy: 0.5740 - lr: 7.8125e-04\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7300 - accuracy: 0.9840 - val_loss: 3.2483 - val_accuracy: 0.5845 - lr: 7.8125e-04\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7290 - accuracy: 0.9858 - val_loss: 3.2475 - val_accuracy: 0.5845 - lr: 7.8125e-04\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7268 - accuracy: 0.9859 - val_loss: 3.2684 - val_accuracy: 0.5770 - lr: 7.8125e-04\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7274 - accuracy: 0.9866 - val_loss: 3.2262 - val_accuracy: 0.5815 - lr: 7.8125e-04\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7251 - accuracy: 0.9868 - val_loss: 3.2655 - val_accuracy: 0.5800 - lr: 7.8125e-04\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7245 - accuracy: 0.9852 - val_loss: 3.2440 - val_accuracy: 0.5775 - lr: 7.8125e-04\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7198 - accuracy: 0.9854 - val_loss: 3.2543 - val_accuracy: 0.5790 - lr: 7.8125e-04\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7212 - accuracy: 0.9858 - val_loss: 3.2414 - val_accuracy: 0.5775 - lr: 7.8125e-04\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7249 - accuracy: 0.9847 - val_loss: 3.2046 - val_accuracy: 0.5825 - lr: 7.8125e-04\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7187 - accuracy: 0.9861 - val_loss: 3.2589 - val_accuracy: 0.5740 - lr: 7.8125e-04\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7156 - accuracy: 0.9886 - val_loss: 3.2646 - val_accuracy: 0.5730 - lr: 7.8125e-04\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7147 - accuracy: 0.9860 - val_loss: 3.2634 - val_accuracy: 0.5755 - lr: 7.8125e-04\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7151 - accuracy: 0.9889 - val_loss: 3.2523 - val_accuracy: 0.5745 - lr: 3.9063e-04\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7179 - accuracy: 0.9872 - val_loss: 3.2564 - val_accuracy: 0.5740 - lr: 3.9063e-04\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7156 - accuracy: 0.9869 - val_loss: 3.2534 - val_accuracy: 0.5770 - lr: 3.9063e-04\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7102 - accuracy: 0.9900 - val_loss: 3.2566 - val_accuracy: 0.5760 - lr: 3.9063e-04\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7147 - accuracy: 0.9883 - val_loss: 3.2491 - val_accuracy: 0.5810 - lr: 3.9063e-04\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7195 - accuracy: 0.9862 - val_loss: 3.2569 - val_accuracy: 0.5790 - lr: 3.9063e-04\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7100 - accuracy: 0.9887 - val_loss: 3.2622 - val_accuracy: 0.5810 - lr: 3.9063e-04\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7096 - accuracy: 0.9876 - val_loss: 3.2689 - val_accuracy: 0.5800 - lr: 3.9063e-04\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7090 - accuracy: 0.9877 - val_loss: 3.2897 - val_accuracy: 0.5765 - lr: 3.9063e-04\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7102 - accuracy: 0.9872 - val_loss: 3.2685 - val_accuracy: 0.5780 - lr: 3.9063e-04\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7131 - accuracy: 0.9881 - val_loss: 3.2686 - val_accuracy: 0.5735 - lr: 3.9063e-04\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7062 - accuracy: 0.9884 - val_loss: 3.2758 - val_accuracy: 0.5745 - lr: 3.9063e-04\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7098 - accuracy: 0.9875 - val_loss: 3.2736 - val_accuracy: 0.5795 - lr: 3.9063e-04\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7121 - accuracy: 0.9870 - val_loss: 3.2782 - val_accuracy: 0.5785 - lr: 3.9063e-04\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7157 - accuracy: 0.9855 - val_loss: 3.2604 - val_accuracy: 0.5780 - lr: 3.9063e-04\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6967 - accuracy: 0.9926 - val_loss: 3.2623 - val_accuracy: 0.5755 - lr: 3.9063e-04\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7062 - accuracy: 0.9876 - val_loss: 3.2676 - val_accuracy: 0.5775 - lr: 3.9063e-04\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7040 - accuracy: 0.9889 - val_loss: 3.2672 - val_accuracy: 0.5790 - lr: 3.9063e-04\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7025 - accuracy: 0.9895 - val_loss: 3.2785 - val_accuracy: 0.5795 - lr: 3.9063e-04\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7036 - accuracy: 0.9886 - val_loss: 3.2832 - val_accuracy: 0.5745 - lr: 3.9063e-04\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7040 - accuracy: 0.9890 - val_loss: 3.2689 - val_accuracy: 0.5775 - lr: 1.9531e-04\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7011 - accuracy: 0.9899 - val_loss: 3.2719 - val_accuracy: 0.5730 - lr: 1.9531e-04\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7023 - accuracy: 0.9896 - val_loss: 3.2641 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7026 - accuracy: 0.9894 - val_loss: 3.2568 - val_accuracy: 0.5805 - lr: 1.9531e-04\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7033 - accuracy: 0.9890 - val_loss: 3.2630 - val_accuracy: 0.5780 - lr: 1.9531e-04\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7029 - accuracy: 0.9895 - val_loss: 3.2650 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7057 - accuracy: 0.9876 - val_loss: 3.2671 - val_accuracy: 0.5810 - lr: 1.9531e-04\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7074 - accuracy: 0.9868 - val_loss: 3.2672 - val_accuracy: 0.5760 - lr: 1.9531e-04\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7014 - accuracy: 0.9879 - val_loss: 3.2663 - val_accuracy: 0.5780 - lr: 1.9531e-04\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6990 - accuracy: 0.9895 - val_loss: 3.2581 - val_accuracy: 0.5790 - lr: 1.9531e-04\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7013 - accuracy: 0.9891 - val_loss: 3.2670 - val_accuracy: 0.5810 - lr: 1.9531e-04\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.7009 - accuracy: 0.9889 - val_loss: 3.2771 - val_accuracy: 0.5760 - lr: 1.9531e-04\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7023 - accuracy: 0.9893 - val_loss: 3.2740 - val_accuracy: 0.5805 - lr: 1.9531e-04\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6938 - accuracy: 0.9929 - val_loss: 3.2632 - val_accuracy: 0.5790 - lr: 1.9531e-04\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.6968 - accuracy: 0.9910 - val_loss: 3.2745 - val_accuracy: 0.5775 - lr: 1.9531e-04\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6976 - accuracy: 0.9904 - val_loss: 3.2859 - val_accuracy: 0.5755 - lr: 1.9531e-04\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7010 - accuracy: 0.9891 - val_loss: 3.2755 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6978 - accuracy: 0.9908 - val_loss: 3.2768 - val_accuracy: 0.5770 - lr: 1.9531e-04\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.7051 - accuracy: 0.9861 - val_loss: 3.2651 - val_accuracy: 0.5800 - lr: 1.9531e-04\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.6960 - accuracy: 0.9908 - val_loss: 3.2685 - val_accuracy: 0.5785 - lr: 1.9531e-04\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 3.2685 - accuracy: 0.5785\n",
      "Score for fold 5: loss of 3.2685134410858154; accuracy of 57.8499972820282%\n"
     ]
    }
   ],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True,random_state = 5)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    #print(train)\n",
    "    #print(test)\n",
    "    model=model_fn()\n",
    "    \n",
    "    # Compile the model\n",
    "    sgd = SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                optimizer=sgd,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=maxepoches,\n",
    "                  validation_data=(inputs[test], targets[test]),\n",
    "                  callbacks=[reduce_lr],\n",
    "                  verbose=verbosity)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=1)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fafce3c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 3.2893881797790527 - Accuracy: 57.89999961853027%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 3.384105920791626 - Accuracy: 56.950002908706665%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 3.2698564529418945 - Accuracy: 59.64999794960022%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.330843925476074 - Accuracy: 56.09999895095825%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 3.2685134410858154 - Accuracy: 57.8499972820282%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 57.68999934196472 (+- 1.1821158964797511)\n",
      "> Loss: 3.3085415840148924\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
